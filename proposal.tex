\documentclass[]{usiinfdocprop}
\usepackage{lipsum}

% TODO remove this
\usepackage{comment}

%% from the book %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{amsthm}

\newcommand{\ml}       {\textsc{Matlab}}
\newcommand{\blas}       {\textsc{Blas}}
\newcommand{\lapack}       {\textsc{Lapack}}
\newcommand{\nl}{\scriptstyle{0}}
\newcommand{\mmd}       {\textsc{Mmd}}
\newcommand{\amd}       {\textsc{Amd}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\colamd}       {\textsc{Colamd}}
\newcommand{\rcm}       {\textsc{Rcm}}
\newcommand{\metis}     {\textsc{Metis}}
\newcommand{\parmetis}     {\textsc{ParMetis}}
\newcommand{\mtmetis}     {\textsc{MT-Metis}}
\newcommand{\scotch}     {\textsc{Scotch}}
\newcommand{\mat}[1]{\left(\begin{array}{#1}}
\newcommand{\rix}{\end{array}\right)}
\newcommand{\R}{\mathbb{R}}
\newcommand{\circn}[1]{{\Large$\bigcirc\mkern-12.0mu$}$#1\;$}
% bold math and symbol, e.g. for vector
%\newcommand{\bm}[1]{\textbf{#1}}
%\newcommand{\bs}[1]{\boldsymbol{#1}}
%
%\newcommand{\mctwo}[1]{\multicolumn{2}{c}{#1}}
%\newcommand{\mltwo}[1]{\multicolumn{2}{l|}{#1}}
%
%\newcommand{\nxlnz}{xl}
%\newcommand{\nlnz}{l}
%\newcommand{\nindx}{id}
%\newcommand{\nxindx}{xid}
%\newcommand{\nr}{r}
%\newcommand{\ntemp}{t}
%
%\newcommand{\vxlnz}{\texttt{\nxlnz{}}}
%\newcommand{\vlnz}{\texttt{\nlnz}}
%\newcommand{\vxindx}{\texttt{\nxindx}}
%\newcommand{\vindx}{\texttt{\nindx}}
%\newcommand{\vr}{\texttt{\nr}}
%\newcommand{\vtemp}{\texttt{\ntemp}}
%
%\newcommand{\cyw}{\text{cy}}
%\newcommand{\mv}[1]{\textbf{#1}}
%
%\newcommand{\bi}{\begin{itemize}}
%\newcommand{\ei}{\end{itemize}}
%
%\newcommand{\be}{\begin{equation}}
%\newcommand{\ee}{\end{equation}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% I hope this works %%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{amsthm}
%\theoremstyle{definition}
%\newenvironment{definition}[1][section]{Definition: #1}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
%\theoremstyle{definition}

\def\formtmp#1#2{{\vskip12pt\noindent\fboxsep=0pt\colorbox{#1}{\vbox{\vskip3pt\hbox to \textwidth{\hskip3pt\vbox{\raggedright\noindent\textbf{#2\vphantom{Qy}}}\hfill}\vspace*{3pt}}}\par\vskip2pt%
\noindent\kern0pt}}

\RequirePackage[x11names]{xcolor}
\definecolor{example}{gray}{0.85}
%\newenvironment{example}[1]{\ignorespaces\def\stmtopen##1{##1}%
%\formtmp{example}{#1}}{\par\noindent\textcolor{example}{\rule{\columnwidth}{1pt}}\vskip2pt\par\addvspace{\baselineskip}}%
\newenvironment{example}[1]{\ignorespaces\def\stmtopen##1{##1}%
\formtmp{example}{#1}}{\par\noindent\textcolor{example}{\rule{\columnwidth}{1pt}}\vskip2pt\par\addvspace{\baselineskip}}%

\definecolor{programcode}{gray}{0.65}
\newenvironment{programcode}[1]{\ignorespaces\def\stmtopen##1{##1}%
\formtmp{programcode}{#1}}{\noindent\textcolor{programcode}{\rule{\columnwidth}{1pt}}\vskip2pt\par\addvspace{\baselineskip}}%

\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}


%4-class OrRd Tokyo SIAM PP18
\definecolor{mycolor0}{HTML}{2B83BA}
\definecolor{mycolor1}{HTML}{D7301F}
\definecolor{mycolor2}{HTML}{FC8D59}
\definecolor{mycolor3}{HTML}{abdda4}
\definecolor{mycolor4}{HTML}{e9a3c9}
\definecolor{mycolor5}{HTML}{FEF0D9}


\usepackage{wrapfig}


\usepackage{graphicx}
\usepackage[caption=false]{subfig}
\usepackage{sidecap}

%\newcommand{\panelsize}{s}

\usepackage{algorithm}
\usepackage{algpseudocode} % part of the algorithmicx package
  % Indent each nesting level only by 1em 
  \algrenewcommand\algorithmicindent{1em}%
  
  
%\usepackage[
%  debug,
%  a4paper, 
%  colorlinks=true, 
%  linkcolor=blue, 
%  citecolor=blue, 
%  urlcolor=blue, 
%  bookmarksopen=true, 
%  bookmarksnumbered=true
%]{hyperref}

\newcommand{\xximg}[3]{%
  \subfloat[#2]{%
    \includegraphics[height=3cm,clip=true]{{{images/perf/p-80/p-#1-#3}}}%
  } \,%
}

\newcommand{\perfimages}[1]{
  \xximg{#1}{lapl1}{n-40-b-4}
  \xximg{#1}{lapl2}{n-70-b-1}
  \xximg{#1}{omen1}{omen-rgf-tc2.5-lc160}
  \xximg{#1}{omen2}{omen-rgf-tc3.5}
  \xximg{#1}{omen3}{omen-rgf-tc4.5}
}

\newcommand{\ximg}[3]{%
  \subfloat[#1]{%
    \includegraphics[height=3cm,clip=true]{{{images/perf/p-80/p-#3-#2}}}%
  } \,%
}

\newcommand{\pimages}[2]{
  \ximg{#1}{#2}{emmy}%
  \ximg{#1}{#2}{hasep1}%
  \ximg{#1}{#2}{woody-hsw}%
  \ximg{#1}{#2}{meggie}%
  \ximg{#1}{#2}{knightmare1}%
  \ximg{#1}{#2}{summitridge1}%
  \ximg{#1}{#2}{sxace}

}

\newcommand{\mv}[1]{\textbf{#1}}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\newcommand{\todo}[1]{ {\quad\color{red}!!!#1!!!\quad} }

%\newcommand{\doiurl}[2][]{#1\href{http://dx.doi.org/#2}{\nolinkurl{#2}}}
%%\newcommand{\bibdoiurl}[1]{\doiurl[doi:]{#1}}
%\newcommand{\bibdoiurl}[1]{DOI:#1}

\newcommand{\arxurl}[2][]{#1\href{http://arxiv.org/abs/#2}{\nolinkurl{#2}}}
% \newcommand{\bibarxurl}[1]{\arxurl[arXiv:]{#1}}
\newcommand{\bibarxurl}[1]{arXiv:#1}

% bold math and symbol, e.g. for vector
\newcommand{\bm}[1]{\textbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}

\newcommand{\mymat}[1]{\textit{#1}}

\newcommand{\mctwo}[1]{\multicolumn{2}{c}{#1}}
%\newcommand{\mltwo}[1]{\multicolumn{2}{l|}{#1}}
\newcommand{\mltwo}[1]{\multicolumn{2}{l}{#1}}
\newcommand{\mlfour}[1]{\multicolumn{4}{l}{\scriptsize #1}}
\newcommand{\mcct}[1]{\multicolumn{3}{c}{#1}}

%\usepackage{ulem} % sout uwave


\newcommand{\nxlnz}{xl}
\newcommand{\nlnz}{l}
\newcommand{\nindx}{id}
\newcommand{\nxindx}{xid}
\newcommand{\nr}{r}
\newcommand{\ntemp}{t}

\newcommand{\vxlnz}{\texttt{\nxlnz{}}}
\newcommand{\vlnz}{\texttt{\nlnz}}
\newcommand{\vxindx}{\texttt{\nxindx}}
\newcommand{\vindx}{\texttt{\nindx}}
\newcommand{\vr}{\texttt{\nr}}
\newcommand{\vtemp}{\texttt{\ntemp}}

\newcommand{\cyw}{\text{cy}}

% Symbol used to denote the panel size.
\newcommand{\panelsize}{s}

\setlength{\tabcolsep}{1pt}


\usepackage{algorithmicx}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{committee}
  \advisor{Prof.}{Olaf}{Schenk}{Universit\`a della Svizzera Italiana, Switzerland}
  \internalmember{Prof.}{Illia}{Horenko}
  \internalmember{Prof.}{Igor}{Pivkin}
  \externalmember{Prof.}{Harald}{K\"ostler}{Department of Computer Science, Friedrichâ€“Alexander University Erlangen-Nurnberg, Germany}
  \externalmember{Prof.}{Tom\'a\v{s}}{Kozubek}{Czech National Supercomputing Center, Technical University of Ostrava, Czech Republic}
\end{committee}
\phddirector{Prof. Olaf Schenk}
\author{Radim Janal\'ik}
\title{Node-Level Performance Modeling \\of Sparse Factorization Solver}
%\subtitle{subtitle}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
%\usetikzlibrary{automata}
\usetikzlibrary{patterns}
\usepackage{float}
\usepackage{listings}

\usepackage{color}
\newcommand{\todol}[1]{\textbf{\textcolor{red}{[[#1]]}}}
\newcommand{\todop}[1]{\noindent\textbf{\textcolor{red}{[[#1]]}}\\}

\newcommand{\doiurl}[2][]{#1\href{http://dx.doi.org/#2}{\nolinkurl{#2}}}
\newcommand{\bibdoiurl}[1]{\doiurl[doi:]{#1}}

%change 'sl' to 'bf' for bold, or 'normalfont' for no special
%formatting
\captionsetup{labelfont={sl,sf}}

\lstdefinelanguage{algebra}
{morekeywords={import,sort,constructors,observers,transformers,axioms,if,
else,end},
sensitive=false,
morecomment=[l]{//s},
}
\abstract{
%Solving large sparse linear systems is at the heart of many application problems arising from engineering problems. These systems are often solved by direct solver, especially when the system needs to be solved for multiple right hand sides or when high numerical precision is required.
%Direct solvers are based on matrix factorization, which is then followed by forward and backward substitution to obtain a solution. The factorization is the most computationally intensive step, but it has to be computed only once for a given matrix. Then the system is solved with forward and backward substitution for every right hand side.
%Performance modelling of algorithms involved in solving linear systems reveals the bottlenecks, which can guide optimizations and shows the best performance that can be achived.
%The Roofline model is widely used to predict upper bound of a code based on processor peak performance and memory bandwidth. Modification of the Roofline model allows us to model performance of forward and backward substitution code, which is combination of sequential and parallel execution. The model predictions are compared with a measurement for representative set of sparse matrices on different x86\_64 processors.
%
Solving large sparse linear systems is at the heart of many application problems arising from scientific and engineering problems. These systems are often solved by direct factorization solvers, especially when the system needs to be solved for multiple right-hand sides or when a high numerical precision is required.
Direct solvers are based on matrix factorization, which is then followed by forward and backward substitution to obtain a precise solution. The factorization is the most computationally intensive step, but it has to be computed only once for a given matrix. Then the system is solved with forward and backward substitution for every right-hand side.
Performance modeling of algorithms involved in solving these linear systems reveals the computational bottlenecks, which can guide node-level performance optimizations and shows the best performance that can be achieved.
The Berkeley roofline model is widely used to predict the upper bound of a code based on processor peak performance and memory bandwidth. Modification of the Roofline model allows us to model performance of forward and backward substitution codes, which are a combination of sequential and parallel execution. The model predictions are compared with various measurements for a representative set of sparse matrices on different x86\_64 processors.
}
\begin{document}
\maketitle
\frontmatter
\tableofcontents
\mainmatter

%\include{00}
%\include{01}
\include{02}
\include{03}
\include{04}
\include{05}
\include{06}
\include{07}
\include{08}
%\include{09}
%\include{99todo}

%\chapter{\todol{Matrices (applications)}}

%todo comment
\begin{comment}
\chapter{\todol{article: Multicore Performance Engineering of Sparse Triangular Solves Using a Modified Roofline Model}}

\begin{comment}
\section{introduction}

high-performance sparse solver libraries have been a very important part of
scientific and engineering computing for years, and their importance
continues to grow as microprocessor architectures become more complex
and software libraries become better designed to integrate easily
within applications. despite the fact that there are various science
and engineering applications, the underlying algorithms typically have
remarkable similarities, especially those algorithms that are most
challenging to implement well in parallel. it is not too strong a
statement to say that these software libraries are essential to the
broad success of scalable high-performance computing in computational
sciences. the recent trends in hardware development have added
additional questions to this scenario, because today's codes are not
guaranteed to exploit the performance of next-generation hardware to a
satisfying degree. the so-called memory wall, i.e., the increasing
performance gap between memory access and processor speed, will force
scientific computing software to deal with the efficient use of
complex multiple memory hierarchies. in addition, many-core
architectures with hundreds of cores using a decreased processor clock
rate will add additional algorithmic and software challenges to
scientific computing.

\begin{figure}[t]
    \centering
  \subfloat[forward subst.]{
        \includegraphics[width=0.25\textwidth,clip=true]{images/forward-small}
  } \, 
  \subfloat[backward subst.]{
        \includegraphics[width=0.25\textwidth,clip=true]{images/backward-small}
  }
  \caption{sparse triangular substitution in compressed sparse column format.}
  \label{algo:triangular}
\end{figure}


in this work we will focus on systems of linear equations that arise
at the heart of many scientific and engineering applications. many of
these linear systems are sparse, i.\,e., most of the elements in the
coefficient matrix are zero. direct methods based on matrix
factorizations are sometimes needed to ensure accurate solutions. for
example, accurate solution of sparse linear systems is needed in
shift-invert lanczos to compute interior eigenvalues. the performance
and resource usage of sparse matrix factorizations are critical to
time-to-solution and maximum problem size solvable on a given
platform. in many applications, the coefficient matrices are
symmetric, and exploiting symmetry will reduce both the amount of work
and storage cost required for factorization. we consider direct
methods for solving a sparse symmetric linear
system, which are based on cholesky or $ldl^t$
factorization. while direct
methods can be expensive for large matrices, in terms of execution
times and storage requirement when compared to iterative methods, they
have the advantage that they terminate in a finite number of
operations. also, direct methods can handle linear systems that are
ill conditioned or the situation when there are many multiple
right-hand sides.

we are, in particular, interested in the forward and backward solution
process of sparse direct solvers since they build the computational
kernel, e.g., in feti-dp methods~\cite{klawonn2002dual}. 
feti-dp are known to be highly
parallelizable, but all implementations are using sparse direct
solvers as building blocks on each compute node in order to
efficiently solve the coarse grid.  
we will investigate and analyze
the performance of the forward/backward solution process of
pardiso~\cite{schenk-2004} 
and present not only numerical
results but also a detailed performance analysis for its sparse solver kernel.
this is based on the roofline model~\cite{williams-2009} 
model~\cite{williams-2009}, a resource-driven and
practically applicable model, and describe the performance of sparse
kernel loops~\cite{gropp99,williams:eecs-2008-164,khwpaf14}.

% the performance of the forward/backward solution process of libraries
% such as cholmod~\cite{doi:10.1137/1.9780898718881},
% mumps~\cite{amestoy2001}, and pardiso~\cite{schenk-2004}, 
% and present not only numerical
% results but also a detailed performance analysis for a representative sparse solver kernel
% based on the execution-cache-memory (ecm)
% model~\cite{treibig-2010-ecm,hager-2012-ecm,stengel-2015}. until recently, the roofline
% model~\cite{williams-2009} was the only resource-driven and
% practically applicable model to describe the performance of sparse
% kernel loops~\cite{gropp99,williams:eecs-2008-164,khwpaf14}.



%\subsection*{related work}

analytical performance models allow us to gather insight about how much
performance to expect and to determine bottlenecks.  for decades,
traditionally, the roofline
model~\cite{callahan88,hockney89,schoenauer00,williams-2009} is used
for this task and relates the arithmetic intensity of a code with the
hardware's capabilities.

\todop{ueberarbeiten, wenn paper fertig.}
the paper is structured as follows. in section~\ref{sec:algo} we
describe sparse direct factorization solvers and emphasize the
algorithms and underlying data structures. section~\ref{sec:tb}
introduces the microarchitectures of the test machine used for
measurements and performance modeling.  section~\ref{sec:pm}
introduces and refines the ecm model on the example of sparse
triangular loop kernels. in section~\ref{sec:pa} we apply
the model to sparse triangular solvers to show how it can identify
bottlenecks, reveal optimization opportunities, and prevent
misleading conclusions. finally, we summarize our findings in
section~\ref{sec:conclusion}.


\section{algorithm and data structures of sparse direct solve}
\label{sec:algo}

let $a$ be an $n \times n$ matrix and $x$ and $b$ be vectors of size $n$.
the linear system $a x = b$ is solved via $ldl^t$ decomposition by factorizing $a$
into a lower diagonal matrix $l$ and a diagonal matrix $d$ so that $a =
ldl^t$.
the system is then solved in three steps. first,
$
  \label{eq:fw}
  ly=b
$
is solved via forward substitution. this is followed by a diagonal solve
$
  \label{eq:fw}
  dz=y
$
and afterwards the resulting $z$ vector is used to solve for the solution vector 
%
$
  \label{eq:bw}
  l^tx=z
$
% 
% via backward substitution. in sparse solver packages, such as, e.g.,
% pardiso~\cite{schenk-2004}, 
via backward substitution. in 
pardiso 
the forward substitution is performed column-wise, starting with the
first column, as depicted in figure~\ref{algo:triangular}.
the data dependencies here allow one to store vectors $y$, $z$, $b$, and $x$ in
only one vector $r$. 
when column $j$ is reached, $r_j$ contains the solution for $y_j$. 
all other elements of $l$ in this column, i.e.,\ $l_{ij}$ with $i = j + 1,
\ldots, n$, are used to update the remaining entries in $r$ by 
%
\be
  r_i = r_i - r_j l_{ij}.
  \label{eq:algo:fw:pardiso}
\ee
%
the backward substitution with~$l^t$ would take place rowwise, but we
use $l$ and perform the substitution columnwise, as shown in the lower part of
figure~\ref{algo:triangular}.  in contrast to the forward substitution the
iteration over columns starts at the last column $n$ and proceeds to
the first one.  
if column $j$ is reached, then $r_j$, which contains the $j$-component of the
solution vector $x_j$, is computed by subtracting the dotproduct of the
remaining elements in the column $l_{ij}$ and the corresponding elements of
$r_i$ with $i = j + 1, \ldots, n$ from it:
%
\be
  r_j = r_j - r_i l_{ij} .
  \label{eq:algo:bw:pardiso}
\ee
%
after all columns have been processed $r$ contains the required solution of $x$.
it is important to note that line 5 represents in both substitutions an indexed
daxpy kernel operation that has to be computed during the streaming operations
of the vector $r$ and the column $j$ of the numerical factor $l$.

as we are dealing with sparse matrices it makes no sense to store the lower
triangular matrix $l$ as a dense matrix.
hence pardiso uses its own data structure to store $l$, as shown in
figure~\ref{fig:algo:ds}. 
in the following we discuss pardiso's data structures for sparse triangular
solve, which are relevant for the algorithm analysis in the next section.

\begin{figure}[t]
  \centering
    \includegraphics[width=0.4\textwidth,clip=true]{images/parts-panels-separator}
  \caption{sparse matrix data structures in pardiso. adjacent columns of $l$ exhibiting the same
structure form panels also known as supernodes. 
groups of panels which touch independent elements of the right-hand side $r$ are
parts. the last part in the lower triangular matrix $l$ is called the separator.}
  \label{fig:algo:ds}
\end{figure}

adjacent columns exhibiting the same row sparsity structure form a
\textit{panel}, also known as a \textit{supernode}.
a panel's column count is called the \textit{panel size} $\panelsize$.
the columns of a panel are stored consecutively in memory excluding the zero
entries. 
note that columns of panels are padded in the front with zeros so they get the 
same length as the first column inside their panel. 
the padding is of utmost performance for the pardiso solver to use level-2/3
blas and lapack functionalities. please see~\cite{20.500.11850/144477} for more
details.
furthermore panels are stored consecutively in the \vlnz{} array. 
row and column information is now stored in accompanying arrays.
the \texttt{xsuper} array stores for each panel the index of its first column. 
%also note that here column indices are the running count of nonzero columns.
column indices are used as indices into the \vxlnz{} array to look up the
start of
the column in the \vlnz{} array which contains the numerical values of the factor $l$.
panel~$p$ starts then in \vlnz{} at \vxlnz\texttt{[xsuper[p]]}.
to determine the row index of a column's element an additional array \vindx{} is
used, which holds the row indices for each panel .
the start of a panel inside \vindx{} is found via \vxindx{} array.
the first row index of panel~$p$ is \vindx\texttt{[\vxindx[p]]}.

for serial execution this information is enough. 
however, during parallel forward/backward substitution concurrent updates to
the same entry of \vr{} must be avoided.
the \textit{parts} structure contains the start (and end) indices of the panels which can
be updated independently as they do not touch the same entries of $r$.
two parts, colored blue and green, are shown in figure~\ref{fig:algo:ds}.
the last part in the bottom right corner of $l$ is special and is called the 
\textit{separator} and is colored gray.
%
parts which would touch entries of \vr{} in the range of the separator perform 
their updates into separate temporary arrays \vtemp{}.
before the separator is then serially updated, the results of the temporary
arrays are gathered back into \vr{}. 
the backward substitution works the same, just reversed, and
only updates to different temporary arrays are not required.

\begin{algorithm}[t]
  \begin{algorithmic}[1]
    \Procedure{sparse forward substitution}{}
      \For{part in parts} \Comment{parallel execution}
        \For{panel p in part}
          \For{\textcolor{blue}{column $j$ in panel p}} \Comment{unroll} \label{alg:fw:1}
            \State i = \nxindx{}[p] + offset
          
            \For{k = \nxlnz[j] + offset; k < sep; ++k}\label{algo:fw:rloop}
                \State row = \nindx[i++]
                \State \nr[row] - =  \nr[j] \nlnz[k] \Comment{indexed daxpy} 
            \EndFor\label{algo:fw:rloop:end}
            \For{k = sep + 1; k < \nxlnz[j+1]; ++k}\label{algo:fw:seploop}
                \State row = \nindx[i++]
                \State \ntemp[row,p] -=  \nr[j] \nlnz[k] \Comment{indexed daxpy}
            \EndFor\label{algo:fw:seploop:end}
          \EndFor
        \EndFor
      \EndFor

      \State
      \State r[i] = r[i] - sum(\ntemp[i,:])  \Comment{gather temporary arrays}
      \State

      \For{panel p in separator} \Comment{serial execution}
        \For{\textcolor{blue}{column $j$ in panel p}} \Comment{unroll}\label{alg:fw:2}
            \State i = \nxindx[p] + offset
          
            \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; ++k}
                \State row = \nindx[i++]
                \State \nr[row] -=  \nr[j] \nlnz[k] \Comment{indexed daxpy}
            \EndFor
        \EndFor
      \EndFor
    \EndProcedure
  \end{algorithmic}
  \caption{forward substitution in pardiso. note that in the case of serial
execution, separated updates to temporary arrays in lines
\ref{algo:fw:seploop}--\ref{algo:fw:seploop:end} are not necessary
and can be handled via the loop in lines
\ref{algo:fw:rloop}--\ref{algo:fw:rloop:end}.}
  \label{alg:algo:fw}
\end{algorithm}

\begin{algorithm}[tp]
   \begin{algorithmic}[1]
     \Procedure{sparse backward substitution}{}
       \For{panel $p$ in sep. rev.} \Comment{serial execution}
         \For{\textcolor{blue}{col. $j$ in panel $p$ rev.}} \Comment{unroll}\label{alg:bw:1}
            \State i = \nxindx[p] + offset
        
            \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; ++k}
                \State row = \nindx[i++]
                \State \nr[j] -= \nr[row] \nlnz[k] \Comment{indexed daxpy}
            \EndFor

            \State offset = offset - 1
          \EndFor
        \EndFor

        \State
 
        \For{part in parts} \Comment{parallel execution}
          \For{panel $p$ in part rev.}
            \For{\textcolor{blue}{col. $j$ in panel $p$ rev.}} \Comment{unroll}\label{alg:bw:2}

              \State i = \nxindx[p] + offset

              \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; ++k}
                \State row = \nindx[i++]
                \State \nr[j] -=  \nr[row] \nlnz[k] \Comment{indexed daxpy}
              \EndFor

              \State offset = offset - 1

            \EndFor
          \EndFor
        \EndFor
        \EndProcedure
   \end{algorithmic}
   \caption{backward substitution in pardiso. separator (sep.), parts, and
panels are iterated over in reversed (rev.) order.}
   \label{alg:algo:bw}
\end{algorithm}


% the complete forward backward substitution is listed in
% algorithm~\ref{alg:algo:fw} and~\ref{alg:algo:bw}, respectively.
% if no parallel execution is required then panels are updated successively in
% serial and during forward substitution updates to temporary arrays are not
% necessary.
the complete forward substitution is listed in algorithm~\ref{alg:algo:fw}.
if no parallel execution is required then panels are updated successively in
serial, and during forward substitution, updates to temporary arrays are not
necessary.
% for performance reasons (discussed later in section~\ref{sec:pam}) the loops over the
% columns (blue text) in algorithms~\ref{alg:algo:fw} and~\ref{alg:algo:bw} are $1$-, $2$-,
% and $8$-way unrolled.
for performance reasons (discussed later in sect.~\ref{sec:pm:dt:wu}) the
loops over the columns (blue text) in algorithm~\ref{alg:algo:fw} are $1$-,
$2$-, and $8$-way unrolled.
the algorithm for backward substitution in algorithm~\ref{alg:algo:bw} 
looks nearly the same, except that first
the serial part is executed, which is then followed by the parallel section.
%


\section{analysis of sparse triangular solve} 
\label{sec:sds}

in this section we analyse the data transferred between the different cache levels
and the floating point operations performed during sparse solve which are used
as input for our performance model established in the next section.
%
therefore we analyze the different instantiations of the loops resulting from
algorithm~\ref{alg:algo:fw} and~\ref{alg:algo:bw}.
hereby we only consider on the innermost loops and do not 
distinguish between updates of~\vr{} or temporary arrays~\vtemp{}.

all coefficients of the $l$ matrix are stored as double-precision floating-point
numbers in \vlnz{}, consuming $8$\,b (byte) each. 
elements of the \vxlnz{} (column start indices in \vlnz{}) and \vxindx{} (start
indices of row indices for each panel) arrays are stored as $8$\,b integers,
whereas for the entries of \vindx{} (row indices for each panel) $4$\,b integers
are used.

\subsection{data transfers without unrolling}
\label{sec:pm:dt}
\label{sec:pm:dt:wou}

\begin{figure}[t]
  \centering
 \includegraphics[width=0.43\textwidth,clip=true]{images/ecm-datatransfers}
  \caption{data transfers for one iteration of the forward
    (including red text and arrows) and backward (without red text and arrows)
    substitution when $1$-, $2$-, or $8$-way column unrolling is applied.
    thereby $1$, $2$, or $8$ nonzero elements of \vlnz{} are processed,
    respectively.
%    loading of \vindx{} from memory (blue boxes) is only included in the model 
%    if the panel size $s \le 8$.
% from which cache level \vr{} is reused (green boxes) depends on the size of the
% active part. if the active part is large it must be reloaded from
% l3 cache. with decreasing size it can fit into l2 or even l1 cache.
% this holds also true for \vindx{}, but additionally depends on the panel size.
  }
  \label{fig:ecm:data}
\end{figure}

in the most simple case no unrolling is applied and the innermost loop of
forward substitution from algorithm~\ref{alg:algo:fw} looks like
%
\begin{algorithmic}[1]
  \setcounter{ALG@line}{22}
  \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; ++k}
      \State row = \nindx[i++]
      \State \nr[row] -= \nr[j] \nlnz[k]
  \EndFor
\end{algorithmic}
as the loops from lines $6$--$9$ and $10$--$13$ are in principal the same as the
on in lines $23$--$26$, we only discuss the latter.
%
during each iteration one nonzero is processed, two floating point operations (flops) are performed,
namely a multiplication and an addition, and the following elements get
loaded and stored, loaded: \vindx{} ($4$~\,b), \vr{} ($8$~\,b), \vlnz{}
($8$~\,b); stored: \vr{} ($8$~\,b).

how much data is transferred inside the cache hierarchy depends on the size of
the caches, their replacement strategies, the size of \vr{}, the average panel
size, as well the structure of the panels.
here we assume \vr{} is small enough to be kept at least in last level cache
(llc) and temporal locality ensures it is not evicted.
%
row indices in \vindx{} for a panel are loaded from memory for the panel's first
column and then are reused during each iteration over the panel's remaining
columns from the llc in the worst case.
with panel size $\panelsize=1$, for each element of \vlnz{} one row index is
transferred and no reuse is possible.
in general reuse is only possible, starting with a panel's second column for
panel sizes $\panelsize \ge 2$.
%
coefficients of \vlnz{} are always streamed in from memory, as they are
used only once and the \vlnz{} array is typically to large to be kept in llc.
figure~\ref{fig:ecm:data} visualizes the transfers assuming 
\vr{} is cached in l3 cache.

during iterating over panels and columns, the number of column elements
decrease as $l$ is a lower triangular matrix.
thereby also the number of used elements from \vr{} decreases, which we call the
active part of \vr{}.
at some point the active part can be completely kept in l2 or even l1 cache.
this also holds true for \vindx{}, except when a new panel starts, then
the panel's row indices must first be loaded from memory.
%
the active part of \vr{} can be kept inside a certain cache size $c_s$, when
concurrently also one column of \vlnz{} and the active part of \vindx{} fit into
this cache level.
let $n_j$ the length of column $j$. 
if $l$ is dense, like for the dense matrix, then if $n_j \le c_s/(8\,\text{b} + 8\,\text{b} +
4\,\text{b})$ holds true all active parts fit into the cache.
%
if instead in the worst case $l$ is sparse so that only one element out of a cache
line from \vr{} is accessed
then $n_j \le c_s/(8\,\text{b} + 64\,\text{b} + 4\,\text{b})$ is required.
%
note that already when the active part of \vr{} slightly exceeds the determined
limits already partial caching in the same cache level takes place\footnote{if
we
load a vector which exceeds the cache size several times it must be completely
reloaded during each iteration. 
however, if the cache utilizes a least recently used policy and the vector's
size is in the range of the cache size $c_s$ and $c_s + w_s$, where $w_s$
denotes the way size of the cache, then still parts of the vector are held in
the cache and need not to be reloaded.
the way size of a cache is defined as the product of the cache's number of sets
and the cache line size.}.

the innermost loop of the backward substitution from
algorithm~\ref{alg:algo:bw}, looks like 
% the innermost loop from lines $5$--$9$ is the same as the one from lines
% $17$--$20$ of the backward
% substitution from algorithm~\ref{alg:algo:bw}, which we only discuss the former,
% which looks like 
%
\begin{algorithmic}[1]
\setcounter{ALG@line}{4}
  \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; ++k}
    \State row = \nindx[i++]
    \State \nr[j] = \nr[j] - \nr[row] \nlnz[k]
  \EndFor
\end{algorithmic}
%
as this loop from lines $5$--$8$ is the same as the one from lines $17$--$20$,
all following statements hold true for both. 
%
as with forward substitution one nonzero is processed, two flops are performed,
but only loads occur: \vindx{} ($4$~\,b), \vr{} ($8$~\,b), and \vlnz{} ($8$~\,b).
note that $j$ is unchanged in the inner most loop, hence $\nr[j]$ always refers
to the same element and is not considered for the data transfer analysis.
%
figure~\ref{fig:ecm:data} displays the data transfers occurring for one
nonzero update, if \vr{} is cached in l3 cache.

\subsection{data transfers with unrolling}
\label{sec:pm:dt:wu}
%
as noted in sect.~\ref{sec:algo} it is beneficial to handle several columns
at once.
in pardiso loops over columns are additionally to no unrolling $2$- and
$8$-times unrolled and used when a panel contains more than one column. 
with an unrolling factor of two the inner most loop for forward substitution
becomes
%
\begin{algorithmic}[1]
  \State nj = nonzero column length
  \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; \textcolor{blue}{k += 2}}
      \State row = \nindx[i++]
      \State \nr[row] = \nr[row] - \nr[j] \nlnz[k] \textcolor{blue}{- \nr[j+1] \nlnz[k+nj]}
  \EndFor
\end{algorithmic}
%
in contrast to no unrolling per iteration two nonzeros are processed.
hence, four flops are performed and two entries of \vlnz{} are loaded.
hereby the corresponding element of \vr{} must only be loaded once instead of
twice, when processing two elements of \vlnz{}.
all other transfers stay unchanged.
%
in general with an $u$-way unrolling during each iteration $2 \times u$\,flops
are executed and $u \times 8\,\text{b} + 20\,\text{b}$ are transferred.

unrolling of the backward substitution loop results in the following code for a
two-way unrolling
%
\begin{algorithmic}[1]
  \State nj = nonzero column length
  \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; \textcolor{blue}{k += 2}}
      \State row = \nindx[i++]
      \State \nr[j]   = \nr[j] - \nlnz[k] \nr[row]
      \State \textcolor{blue}{\nr[j+1]   = \nr[j+1] - \nlnz[k+nj] \nr[row]}
  \EndFor
\end{algorithmic}
%
also here four flops per iteration are performed, two entries of \vlnz{} are
loaded and the corresponding element of \vr{} is loaded only once.
%
for an $u$-way unrolling $2 \times u$\,flops and $u \times 12\,\text{b} +
8\,\text{b}$ are loaded.

as already noted, we assume \vr{} and a panel's current row indices from
\vindx{} are at least cached in llc or higher cache levels.
%
unrolling hereby only saves transfers inside the cache hierarchy.
the bytes transferred between memory and llc are left unaffected and depend only
on the panel size.
with larger panel sizes the in total less row indices \vindx{} are needed for the
whole matrix. 

%todo comment
\end{comment}



\begin{comment}
\section{modified roofline model for sparse direct solve}
\label{sec:mrm}

% \begin{table}[tp]
%   \centering
%   \small
%   \begin{tabular}{l|cccc}
%   \hline
%   u & l1 & l2 & l3 & mem \\
%   \hline
%   forward \\
%   \hline
%   1 & 14   & 4 -- 14  &  4 -- 14  & 4 -- 6    & \\
%   2 &  9   & 4 --  9  &  4 --  9  & 4 -- 5    & \\
%   8 & 5.25 & 4 -- 5.25&  4 -- 5.25& 4 -- 4.25 & \\
%   \hline
%   backward \\
%   \hline
%   1 & 10  & 4 -- 10 & 4 -- 10 & 4 -- 6 \\
%   2 & 7   & 4 --  7 & 4 --  7 & 4 -- 5 \\
%   8 & 4.75& 4 -- 4.75&4--4.75& 4 -- 4.25\\
%   \end{tabular}
%   \caption{code balance $b_c$ [b/flop] of forward/backward substitution for
% different unrollings (u) when
% data is fetched from the corresponding level inside the memory hierarchy. values
% depend on actual panel size $s$ and possible cache reuse.}
%   \label{tab:mrm:bc}
% \end{table}
\begin{table}[tp]
  \centering
  \small
  \begin{tabular}{l|ccccccccccccccccccccccccccccccccc}
  \hline
  \multicolumn{10}{c}{forward}  && \multicolumn{10}{c}{backward} \\
  \cline{2-11} \cline{13-22}
  u & L1 & \mcct{L2} & \mcct{L3} & \mcct{mem} && L1 & \mcct{L2} & \mcct{L3} & \mcct{mem}\\
  \hline
  1 & 14   & 4 & -- & 14  &  4 & -- & 14  & 4 & -- & 6    && 10   & 4 & -- & 10 & 4&  --&  10 & 4 & -- &6 \\ 
  2 &  9   & 4 & -- &  9  &  4 & -- &  9  & 4 & -- & 5    &&  7   & 4 & -- &  7 & 4&  --&   7 & 4 & -- &5 \\
  8 & 5.25 & 4 & -- & 5.25&  4 & -- & 5.25& 4 & -- & 4.25 && 4.75 & 4 & -- & 4.75&4&  --& 4.75& 4 & -- &4.25 \\
  \hline
  \end{tabular}
  \caption{Code balance $B_c$ [B/flop] of forward/backward substitution for
different unrollings (u) when
data is fetched from the corresponding level inside the memory hierarchy. Values
depend on actual panel size $s$ and possible cache reuse.}
  \label{tab:mrm:bc}
\end{table}

we base our performance predictions on the roofline model~\cite{williams-2009}.
the model takes into account the attainable memory bandwidth and peak floating
point performance of the processor and correlates it with the requirements of
the code.
it can be written as
%
\be
  p = \min(p_\text{max}, b / b_c),
\ee
where $p_\text{max}$ denotes the attainable floating point performance, $b$ the
attainable memory bandwidth and $b_c$ the \textit{code balance}.
here $p_\text{max}$
depends already on the floating point characteristics of the code. 
first it depends on the usage of vector or scalar instructions in the code.
in our case only scalar instructions are are used, which for example quarter the
theoretically attainable floating point performance for a processor supporting
avx.
furthermore if floating point units for addition and multiplication are
separated and the code uses only one type of these operation the attainable
performance is furthermore halfed.
%
the attainable memory bandwidth $b$ is measured with a micro-benchmark,
ideally resembling the applications memory access pattern. 
%
the code balance $b_c$ is the ratio of by bytes transferred to the number of
floating point operations performed in the code.
in the best case, when panel size $\panelsize$ is large and the indices are
cached, during forward~\eqref{eq:algo:fw:pardiso} and backward
substitution~\eqref{eq:algo:bw:pardiso} each nonzero of $l$ must be loaded once
and the loading of indices can be neglected,
respectively. 
furthermore the computation involves two floating point operations (flops) per
nonzero.
as nonzeros are stored in double precision consuming $8$\,b, this results in a
best case code balance of $b_c = 8 / 2 \text{b/flop} = 4 \text{b/flop}$.
table~\ref{tab:mrm:bc} lists the code balance for different unrollings and cache
levels and uses the data transfers from sect.~\ref{sec:sds}.
%
in contrast the \textit{machine balance} $b_m$ defines these ratio for the hole system
and uses the ratio of attainable memory bandwidth $b$ to maximum floating point
performance $p_\text{max}$.
this is found in table~\ref{} for all systems.
%
if $b_c > b_m$ then the code's performance is limited by the memory bandwidth,
hence it is memory bound.
this is the case for sparse direct solve for all unrollings, when data is
located in l2, l3, or memory.

to determine performance limits we only consider the case when data resides in
memory.
hence, as fig.\ref{fig:ecm:data} indicates, only the number of nonzeros of $l$ and the
corresponding indices are relevant and is independent of the loop unrollings.
%
however we distinguish between the parallel phase, where the parts are handled,
depending on the number of threads $t$ and the serial part, where the separator
is treated, as they achieve different bandwidths as shown later, separately.
we use the following formula for a certain matrix $a$ factorized and solved with
$t$ threads:
%
\be
  p^{a}(t) 
  = \frac{
      \text{nnz} (l) \times 2 \frac { \text{flop}}{ \text{nz}}
    }{
     \frac{d_a^p(t) }{  b(t) } + \frac{ d_a^s(t) }{ b(1) }
    } \left[ \frac{\text{flop}}{s} \right],
\ee
%
where $\text{nnz}(l)$ denotes the number of nonzeros of the factor $l$ 
resulting from a factorization of $a$ for $t$ threads,
$b(t)$ is the attainable memory bandwidth with $t$ threads,
$d_a^p(t)$ the data volume of nonzeros and indices made up by the parallel 
parts, and $d_a^s(t)$ the data volume made up by the nonzeros and indices of the
separator.
%
please note that both data volumes $d_a^p(t)$ and $d_a^s(t)$ depend on
$a$ and the number of threads $t$.
%
we extract $d_a^p(t)$ and $d_a^s(t)$ from the factorized matrices.
%

% all coefficients of the $l$ matrix are stored as double-precision floating-point
% numbers in \vlnz{}, consuming $8$\,b (byte) each. 
% elements of the \vxlnz{} and \vxindx{} arrays are stored as $8$\,b
% integers, whereas for the entries of \vindx{} $4$\,b integers are used.
% 
% during forward~\eqref{eq:algo:fw:pardiso} and backward
% substitution~\eqref{eq:algo:bw:pardiso} each nonzero of $l$ must be loaded once, 
% respectively. 
% further the computation involves two floating point operations (flops).
% in the best case the panel size $\panelsize$ is large and the indices are cached, hence
% they must only be loaded once for the panel from memory.
% this case has a \textit{code balance}, the ratio of transferred bytes between
% memory and cpu compared to the number of flops performed, of $b_c = 8/2$ b/flop
% $= 4$~b/flop.
% this is clearly higher than the machine balance $b_m$ in table~\ref{tab:hw} of all
% evaluated systems, which indicates that the code is memory-bound.
% 
% for performance prediction $p_\text{rfm}^a(t)$ we model the parallel phase,
% where the parts are handled and the serial part, where the separator is treated
% separately.
% we use the following formula for a certain matrix $a$ solved with $t$ threads:
% %
% \be
%   p_\text{rfm}^{a}(t) 
%   = \frac{
%       \text{nnz} (l) \times 2 \frac { \text{flop}}{ \text{nz}}
%     }{
%      \frac{d_p^a(t) }{  b(t) } + \frac{ d_s^a(t) }{ b(1) }
%     } \left[ \frac{\text{flop}}{s} \right],
% \ee
% %
% where $\text{nnz}(l)$ denotes the number of nonzeros of the factor $l$ from $a$
% \todo{depends this on the number of threads?},
% $b(t)$ is the attainable memory bandwidth with $t$ threads,
% $d_p^a(t)$ the data volume of nonzeros and indices made up by the parallel 
% parts, and $d_s^a(t)$ the data volume made up by the nonzeros and indices of the
% separator.
% please note that for the data volumes $d_p^a(t)$ and $d_s^a(t)$ depend both on
% $a$ and also on the number of threads $t$.
% we extracted the data volumes $d_p^a(t)$ and $d_s^a(t)$ from the factorized
% matrices.
% also note that possible loop unrolling (discussed later in
% section~\ref{sec:pm:dt:wu}) has no impact on the transferred data volume between
% cpu and memory here.

the attainable memory bandwidth depends on the access pattern of the
application.
hence, it is crucial to choose a benchmark for measuring which resembles the
application's behavior for more precise predictions.
in case of sparse direct solve this is a read only benchmark where only one
vector is read from memory. 
%
typically memory bandwidth benchmarks utilize vectorized loads and stores.
however, sparse solve in pardiso only uses scalar instructions as it is not
vectroized by the compiler.

% if enough cores are used then scalar and vectorized benchmarks saturate the
% memory bandwidth with the only difference that the saturation of the latter
% is already achieved with less cores. this is shown in figure~\ref{fig:mrm:bw-scaling} 
% exemplified on the hsw1 and hsw2 system.
% %
% as for our modified roofline model also the single core bandwidth is relevant we
% use for all bandwidth measurements the scalar read only benchmark.
% figure~\ref{fig:mrm:bw-single-core} shows the difference between the scalar and
% vectorized versions for the single core and the full processor or cluster for
% all systems in the test bed (except sx).
% 
% \begin{figure}[tp]
%   \centering
%   \subfloat[]{%
%     \includegraphics[height=0.3\textheight,clip=true]{images/stream/streamreadhswscalarvectorized}
%     \label{fig:mrm:bw-scaling}
%   } \,
%   \subfloat[]{%
%     \includegraphics[height=0.3\textheight,clip=true]{images/stream/streamreadsinglecorefullprocessor}
%     \label{fig:mrm:bw-single-core}
%   }
%   \caption{bandwidth over the number of cores of the read only benchmark in a
% scalar and vectorized version exemplified on hsw-d and
% hsw-s~\protect\subref{fig:mrm:bw-scaling}.
% single core bandwidth and saturated bandwidth with all available cores of the
% processor/cluster~\protect\subref{fig:mrm:bw-single-core}.}
%   \label{fig:mrm:bw}
% \end{figure}






\section {experimental testbed for performance evalutaion}
\label{sec:tb}

\begin{table*}[tp]
 \centering
\resizebox{\textwidth}{!}{%
 \begin{tabular}{lllrrrrrrrrr}
    \hline
    name      & &  & ivb         & hsw-d      & hsw-s           & bdw           & skx         & knl           & zen-d        &  zen-s         & sx \\
    \hline                                                      
    processor & &  & intel xeon  & intel xeon & intel xeon      & intel xeon    & intel xeon  & intel xeon    & amd ryzen    &  amd epyc      & nec \\
    name      & &  &  e5-2660 v2 & e3-1240 v3 & e5-2695 v3      & e5-2630 v4    & gold 6148   & phi 7210      & 7 1700x      &  745           & sx ace    \\
    \hline                                                      
    micro     & &  & ivy bridge  & haswell    & haswell         & broadwell     & skylake     & knigths       & zen          &  zen           & sx ace\\
    arch.     & &  &             &            &                 &               &             & landing       & \\
    \hline                                                    
    freq    & [ghz] & & 2.2      & 3.4        & 2.3             & 2.2           & 2.4         & $\approx$ 1.3 & 3.4          &  2.3           & 1.0   \\
    cores   &       & & 10       & 4          & 2 $\times$ 7    & 10            & 20          & 64            &   8          &  24            & 4 \\
    isa     &       & & avx      & avx2       & avx2            & avx2          & avx-512     & avx-512       & avx2         &  avx2          & -  \\ % latest isa extension
%    sockets &       & 2        & 1          & 2               & 2             & 2           & 1             & 1            &  & 1 \\
%    \mltwo{numa lds}& 2        & 1          & 2 $\times$ 2    & 2 $\times$ 2  & 2           & 1             & 1            &  & 1 \\
    \mltwo{numa lds} & & 1       & 1          & 2               & 1             & 1           & 1             & 1            & 4              & 1 \\
    \hline                                                    
    l1 & [kib]     &  &  32      & 32         & 32              & 32            & 32          & 32            & 32           &  32            & 1024 \\ 
    l2 & [kib]     &  &  256     & 256        & 256             & 256           & 1024        & 1024          & 512          &  512           & - \\
    l3 & [mib]     &  &  25      & 8          & 2 $\times$ 17.5 & 25            & 28          & -             & 2 $\times$ 8 &  8 $\times$ 8  & -\\
    \hline
%    copy bw. & [gb/s] & 41.2   & 26.6            & 22.6       & 31.3 & ?? &  75.9 & 30.2 & 212 \\ % complete socket/cod
%    read bw. & [gb/s] & 44.3   & 30.9            & 23.6       & 33.7 & ?? &  74.2 & 32.5 & 231 \\ % complete socket/cod
    \mltwo{read bw.}   &     &          &  \\
    ~1 core         & [gb/s]  & & 9.7 & 15.8 & 11.0 & 10.1 & 8.4 & 3.0 & 17.2 & 15.8 & 231.0 \\
    ~numa ld& [gb/s]   & & 43.8 & 22.8 & 30.9 & 55.8 & 87.9 & 73.5 & 33.1 & 37.6 & 231.0 \\

    \hline
    \mltwo{machine balance} &  &         & \\
    ~1 core  & [b/f] && 2.2 & 2.3 & 2.4 & 2.2 & 1.8 & 1.1 & 2.5 & 3.4 & 0.9 \\
    ~numa ld & [b/f] && 1.0 & 0.8 & 1.0 & 1.2 & 0.9 & 0.4 & 0.6 & 1.4 & 0.2 \\
    \hline
\\[0.01em]
  \end{tabular}} % from https://tex.stackexchange.com/a/27105
  \caption{details of evaluated hardware systems. isa lists only the latest
extension supported by the processor. 
read bandwidth and machine balance is reported for scalar execution.
knl's bandwidth numbers are for ddr memory.}
% ecm: 2 cy l1/l2 bei hsw/bdw entgegen der doku 
% stream:
% read = summation
% knl: no-nt, prefetch not explicitly disabled
% zen: no-nt, avx2, only even cores
% hsw2: no-nt, avx2
% hsw: no-nt, avx2
% ivb: no-nt, 
  \label{tab:hw}
\end{table*}

as hardware environment for performance analysis we used a variety of machines
with processors from different vendors.
the specifications of the machines are described in table~\ref{tab:hw}. 
%
the machines with intel processors are based on the ivy bridge (ivb), haswell
(hsw-d \& hsw-s), broadwell (bdw), skylake (skx) and knights landing (knl)
microarchitectures. 
the first four microarchitectures are successors to each other and can be seen
as traditional superscalar, multicore, simd capable processors.
hsw-d and hsw-s are desktop and server systems, respectively.
the hsw-s systems has cluster-on-die (cod) enabled. 
here the processor's local l3 cache is divided into two parts and
the memory forms two numa locality domains.
%
skx is the server variant of the skylake microarchitecture including support for
avx-512 and hosts an additional fused-multiply-add (fma) unit.
%
knights landing in contrast to the previous processors, an exemplar of the xeon
phi line, is the successor of knights corner, which exhibits a manycore
architecture with wider simd lanes compared to the formerly named processors.
the exact avx-512 isa for knights lading differs from the skylake incarnation,
but is for our purpose not relevant. 
knights lading includes a $16$\,gb large high bandwidth memory (hbm) with
bandwidths up to $450$\,gb/s.
however, we do not utilize it which is discussed in sect.~\ref{sec:pa}.
we operate knl in the flat memory model, where the ddr memory and hbm 
represent a numa domain, each.
%
from amd a desktop (zen-d) and server (zen-s) system based on the zen
microarchitecture is used. zen-s' processor with $24$ cores consists of four
numa lds.
%
from nec we use a sx-ace (sx) vector system.

the cpu frequencies on all machines were fixed to the base frequencies specified
in table~\ref{tab:hw} where applicable.
on the zen system we set the frequency to the nominal base frequency of
$3.4$\,ghz, but could not disable turbo mode, which might let cores run above
this frequency.
for knights landing and the sx-ace system altering frequencies is not supported.
on knl the frequency is handled by the processor itself and at the sx-ace system
we assume an already fixed frequency.
%
furthermore each thread's affinity was explicitly set.
%
for all arrays large $2$~mib pages were used.
this was achieved by using transparent huge pages
\footnote{\texttt{/sys/kernel/mm/transparent\_hugepage/[enabled|defrag]} were set to
\texttt{always}.} as well as calling \texttt{madvise(madv\_hugepage)} after
allocation of memory.
%
furthermore first-touch policy was in place and we verified via numa-api data
always resides in the cores associated numa domain.
%
as compiler, intel c/fortran compiler version 17.0.1 was used, except for the
sx-ace system where the sx compilers in revision 533 2017/02/24 were utilized.

\subsection{read-only memory bandwidth}

read bandwidth reported in table~\ref{tab:hw} is obtained via a read-only
benchmark using scalar load instructions.
this resembles best sparse solve's memory access pattern as discussed in
sect.~\ref{sec:mrm}.
%
if enough cores are used then scalar and vectorized benchmarks saturate the
memory bandwidth with the only difference that the saturation of the latter
is already achieved with less cores. 
this is shown in figure~\ref{fig:mrm:bw-scaling} exemplified on the hsw-d and
hsw-s system.
comparing the saturation behavior of hsw-d and hsw-s shows the typical behavior
for (intel's) desktop and server systems. 
desktop systems often already saturate the memory bandwidth with one core.
%
figure~\ref{fig:mrm:bw-single-core} shows the difference between the scalar and
vectorized versions of the read-only benchmark for the single core and the full
numa ld on all systems in the test bed except sx. 
skx, knl, and the zen-based\footnote{
for measurements on zen-d only one core per compute complex is used, which
yields a slightly higher bandwidth than utilizing all available cores.
\todo{describe compute complex or remove it.}}
 systems reach with one core and scalar loads a
significantly lower bandwidth than with vectorized instructions.
however, utilizing the full numa ld nearly no difference is visible.

\subsection{machine balance}

the machine balance from tab.~\ref{tab:hw} considers the scalar read-only memory
bandwidths and the scalar double precision floating point capabilities of the
processors as utilized by pardiso's spare solve.
it requires a (scalar) fp addition and multiplication for processing a nonzero,
meaning we need a balanced number of additions and multiplications.
%
comparing the machine balance of all systems to the code balance for sparse
solve from tab.~\ref{tab:mrm:bc} shows that the latter when data resides in
memory is always larger. 
this indicates sparse solve is always memory bound.

\begin{figure}[tp]
  \centering
  \subfloat[]{%
    \includegraphics[height=0.3\textheight,clip=true]{images/stream/streamreadhswscalarvectorized}
    \label{fig:mrm:bw-scaling}
  } \,
  \subfloat[]{%
    \includegraphics[height=0.3\textheight,clip=true]{images/stream/streamreadsinglecorefullprocessor}
    \label{fig:mrm:bw-single-core}
  }
  \caption{bandwidth over the number of cores of the read only benchmark in a
scalar and vectorized version exemplified on hsw-d and
hsw-s~\protect\subref{fig:mrm:bw-scaling}.
single core bandwidth and saturated bandwidth with all available cores of the
processor/cluster~\protect\subref{fig:mrm:bw-single-core}.}
  \label{fig:mrm:bw}
\end{figure}
% the bandwidth between each cache level also shown in table~\ref{tab:hw} is
% obtained from~\ref{intel-orm-2016}. 
% despite the documentation reports $1$\,cy/cl between l1- and l2-cache for the
% haswell microarchitecture, only around  half the bandwidth is observed to be
% reached~\cite{hofmann-2016-hsw}.
% hence, we assume $2$\,cy/cl as bandwidth.

\begin{table}[tp]
  \centering
  \small
  \begin{tabular}{l|rrr}
  \hline
         &  \multicolumn{1}{c}{$\bm{n}$} &
            \multicolumn{1}{c}{$\bm{\text{nnz}(a)}$}  &
            \multicolumn{1}{c}{$\bm{\text{nnz}(l)}$}   \\ 
  \hline
% values for threads = 1, p = 80
  dense  & $    20 \times 10^3$ & $200 \times 10^6$ &  $  200 \times 10^6$  \\ % ps-n-20000-t-1-p-80
  lapl1  & $   256 \times 10^3$ & $  3 \times 10^6$ &  $  219 \times 10^6$  \\ % pl-n-40-b-4-t-1-p-80  n=40, b=4
  lapl2  & $   343 \times 10^3$ & $  1 \times 10^6$ &  $  166 \times 10^6$  \\ % pl-n-70-b-1-t-1-p-80  n=70, b=1
  omen1  & $1\,751 \times 10^3$ & $ 32 \times 10^6$ & $1\,076 \times 10^6$  \\ % omen-rc2.5-lc160-t-1-p-80
  omen2  & $   760 \times 10^3$ & $ 20 \times 10^6$ & $   690 \times 10^6$  \\ % omen-rc3.5-t-1-p-80
  omen3  & $1\,271 \times 10^3$ & $ 42 \times 10^6$ & $1\,651 \times 10^6$  \\ % omen-rc4.5-t-1-p-80
  feti1  & $   750 \times 10^3$ & $ 31 \times 10^6$ & $1\,590 \times 10^6$  \\ % mat\_kii\_sd22\_size750141\_load2\_newton1
  \hline
  \end{tabular}
  \caption{dimension ($n$) and number of nonzeros ($\text{nnz}$) for $a$ and 
$l$ for all benchmark matrices.}
  \label{tab:m:list}
\end{table}

\subsection{matrices}

some information on the symmetric matrices that are used
for the performance modeling experiments in the next sections 
are shown in table~\ref{tab:m:list}. it shows
matrix dimension ($n$), number of nonzeros in the matrix ($\text{nnz}(a)$),  
number of nonzeros in the factor ($\text{nnz}(l)$).
 the reported numbers of nonzeros stem from a factorizations, which are generated
for single threaded execution and panel size $\panelsize = 80$. 
all matrices are stored in compressed sparse row format and all are sparse except 
for the first matrix "dense", where both the matrix and  
the factor $l$ are dense. we use this dense matrix as a best case example 
for our single core performance investigations on indexed daxpy operations.
the matrices ``lapl1'' and ``lapl2'' are test matrices arising from a 
finite difference discretization of the laplace operator in three dimensions
with dirichlet 
boundary conditions. in addition, the matrix ``lapl2'' contains a block structure
of size $4$. the ``omen'' matrices correspond to a set of representative matrices from an atomistic nanoelectronic device engineering simulation code~\cite{luisier2011atomistic}. 
matrix ``feti1'' has been obtained from a finite element 
tearing and interconnecting (feti) code~\cite{klawonn2002dual}.
figure~\ref{fig:m:spy} shows the structure of $a$ for different matrix classes, whereas more  interesting, is the nonzero distribution over the panel sizes of the factor $l$ found in 
figure~\ref{fig:m}.

please note that the current factorization limits the number of parts to powers
of two.
this leads to a load imbalance during the solve step, which is why in this
article we only report results for thread counts which are equal to powers of
two.

\begin{figure*}[tp]
  \centering
  \subfloat[lapl]{%
    \includegraphics[width=0.2\textwidth,clip=true]{images/spy-plots/a-pl-n-8-b-1}
    \label{fig:m:spy:lp}
  } \hspace{1cm}
  \subfloat[omen]{%
    \includegraphics[width=0.2\textwidth,clip=true]{images/spy-plots/omen}
    \label{fig:m:spy:omen}
  } \hspace{1cm}
  \subfloat[feti1]{%
    \includegraphics[width=0.2\textwidth,clip=true]{images/spy-plots/mat_kii_sd89_size10992_load2_newton1}
    \label{fig:m:spy:feti}
  } \,
  \caption{structure of $a$ for matrix classes lapl~\protect\subref{fig:m:spy:lp},
  omen~\protect\subref{fig:m:spy:omen}, and feti~\protect\subref{fig:m:spy:feti}.}
  \label{fig:m:spy}
\end{figure*}

% {{{
% to generate/alter images:
% - go to directory data/matrices
%
% - run: ./plot-log.py <matrix-name>.dat
%   this will generate a pdf and png of the histogram.
%
% - copy the pdf to images/matrices
%
% \begin{figure*}[tp]
%   \centering
%   \subfloat[dense]{%
%     \includegraphics[width=0.3\textwidth,clip=true]{images/matrices/ps-n-20000-t-1-p-80}
%     \label{fig:m:dense}
%   } \,
%   \subfloat[lapl1]{%
%     \includegraphics[width=0.3\textwidth,clip=true]{images/matrices/pl-n-00040-b-004}
%     \label{fig:m:laplace:n40b4}
%   } \,
%   \subfloat[lapl2]{%
%     \includegraphics[width=0.3\textwidth,clip=true]{images/matrices/pl-n-70-b-1-t-1-p-80}
%     \label{fig:m:laplace:n70b1}
%   } \,
%   \subfloat[omen1]{%
%     \includegraphics[width=0.3\textwidth,clip=true]{{{images/matrices/omen-rgf-tc2.5-lc160-t-1-p-80.hist-rhs-update-frequencies}}}
%     \label{fig:m:omen}
%   } \,
%   \subfloat[omen2]{%
%     \includegraphics[width=0.3\textwidth,clip=true]{{{images/matrices/omen-rgf-tc3.5-t-1-p-80.hist-rhs-update-frequencies}}}
%     \label{fig:m:omen}
%   } \,
% %  \subfloat[omen3]{%
% %    \includegraphics[width=0.3\textwidth,clip=true]{{{images/matrices/omen-rgf-tc4.5-t-1-p-80.hist-rhs-update-frequencies}}}
% %    \label{fig:m:omen}
% %  } \,
%   \subfloat[feti1]{%
%     \includegraphics[width=0.3\textwidth,clip=true]{{{images/matrices/mat_kii_sd22_size750141_load2_newton1-t-1-p-80.log-rhs-update-frequencies}}}
%     \label{fig:m:omen}
%   } \,
%   \caption{multi-parameter histograms showing how many nonzeros of $l$ are in panels with a certain
%            number of columns and rows. panels with $3000$ and more rows are
%            accumulated.
%            this plot gives an impression of how the work, i.e.,\ nonzeros,
%            in~$l$ is distributed over panel dimensions.
%   }
%   \label{fig:m}
% \end{figure*}
% }}}





\section{performance evaluation and analysis for sparse triangular solves}
\label{sec:pa}
\label{sec:performance}



\subsection{single core performance discussion}
%
\begin{figure*}[tp]%
  \centering%
\subfloat[ivb]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-emmy-n-20000}
} \,%
\subfloat[hsw-d]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-woody-hsw-n-20000}
} \,%
\subfloat[hsw-s]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-hasep1-n-20000}
} \,%
\subfloat[bdw]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-meggie-n-20000}
} \,%
\subfloat[skx]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-skylakesp2-n-20000}
} \,%
\subfloat[knl]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-knightmare1-n-20000}
} \,%
\subfloat[zen-d]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-summitridge1-n-20000}
} \,%
\subfloat[zen-s]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-naples1-n-20000}
} \,%
\subfloat[sx]{%
\includegraphics[height=6.5cm,clip=true]{images/perf/ps-n-20000/p-single-core-sxace-n-20000}
\label{fig:p:single-core:sx}
}
  \caption{performance of sparse triangular solve for the original pardiso
and the model guided minimal requirements (mr) implementation with $1$-, $2$-, and
$8$-way column unrolling for matrix dense with panel size $\panelsize = 1$, $2$,
and $80$, respectively.
green horizontal bars show the modified roofline model predictions. for sx these
are $38$, $46$, and $57$\,gflop/s for panel size $\panelsize = 1$, $2$,
and $80$, respectively. 
where available, red bars show the ecm model predictions (best and worst case
assumptions).
}%
  \label{fig:p:single-core}%
\end{figure*}

% \begin{figure}[tp]
%   \centering
%   \includegraphics[width=0.2\textwidth,clip=true]{{{images/whiskers-t-fw-bw-cy-nnz-hasep1-f-2.3-ps-n-20000}}}
%   \caption{performance of forward (fw) and backward (bw) substitution for
%   original pardiso on hsw1 for matrix dense.
%   blue and red horizontal bars show the prediction of the ecm model for the best
%   and worst case, respectively.}
%   \label{fig:pa:error}
% \end{figure}


first we discuss the single core performance of the pardiso code and the model
guided minimal requirements (mr) implementation. 
we use matrix dense for evaluation and create three variants of it, where we
limited the maximum panel size $\panelsize$ to $1$, $2$, and $80$ columns.
this enables isolated measurement of the $1$-, $2$-, and $8$-way
unrolled loops.

figure~\ref{fig:p:single-core} shows the measured performance for all systems in
our testbed with the original pardiso code and mr implementation. 
as the pardiso code and the mr implementation have the same code balance the
modified roofline model prediction from section~\ref{sec:mrm} (green horizontal
bars) is the same for both.

\paragraph{ivy bridge, haswell, broadwell based systems}
the modified roofline model predictions for the intel based system (except for
knl) show model errors up to $30$\,\%.
this is expected as the prediction depends on the used benchmark determine the
memory bandwidth which does not exactly resembles the access pattern exhibited by
sparse triangluar solve.
pardiso and mr exhibit the same performance. 
this issue is discussed later.
%
\paragraph{knights landing based system}
for knl the model error is up to $200$\,\%.
in the case of panel size $s = 2$ and $s=80$ mr achieves a higher performance than pardiso.
for mr avx-512 with explicit scatter and gather instructions is used, compared
to pardiso where the compiler does not use this instructions despite
\texttt{-xmic-avx512} is specified.
\todo{describe why}.
%
\paragraph{zen based system}
the roofline model for the amd based zen system also overestimates the
performance by around $150$\,\%. 
interestingly mr with $2$-way unrolling reaches a higher bandwidth than the read benchmark used as
input for the roofline model (bar exceeds the roofline model line).
for both pardiso and mr we used the avx2 instruction set (\texttt{-xcore-avx2})
and overloaded the compiler generated cpu dispatcher.
only for panel size $s=1$ and $s=2$ mr doubles performance. with $s=80$ both
code version are again on par.
%
the cause for reaching a higher bandwidth than read and the twice as high
pardiso performance is unclear and still under investigation.
%
\paragraph{sx system}
for the sx machine we utilized the compiler directive \texttt{nodep} for all
loops in sparse solve, labeled ``pardiso'' (black bars) in
figure~\ref{fig:p:single-core:sx}.
furthermore we introduced an optimized version ``w/ vec.\ opt.'' shown in
in figure~\ref{fig:p:single-core:sx} (orange bars).
it uses besides the \texttt{nodep} directive \texttt{vovertake} and \texttt{vob}
during forward substitution and \texttt{vovertake} during backward
substitution.
%
the modified roofline model predictions exceed the graphs y-axis and are
therefore not shown.
despite the optimized version is significantly faster, the model error is up to
$650$\,\%. 
\todo{reason? partially gather/scatter which is slow?}
as the compiler already utilizes gather and scatter instructions and places
vector $r$ in the adb (assignable data buffer, a $1$\,mib large vector cache) in
both code versions, we did not explicitly include any other optimizations.

%-- \subsubsection*{ecm pardiso}
%-- 
%-- with the ecm model we try to increase the accuracy of the prediction.  as the
%-- actual amount data transferred depends on different factors, as discussed in
%-- section~\ref{sec:pm:dt:wou} and~\ref{sec:pm:dt:wu}.
%-- in figure~\ref{fig:p:single-core} we give best (high red bar) and worst case
%-- (low red bar) values for the model of the pardiso code for architectures where
%-- the ecm model is available.
%-- best case assumes $r$ when accessed is fetched from l1 cache, whereas in the
%-- worst case $r$ must be loaded from $l3$ cache.
%-- for ivb and bdw the ecm model predictions are quite well especially for panel
%-- size $s=1$ and~$2$.
%-- only for panel size $s=80$ the model overestimates the performance.
%-- the hsw systems the performance is in general under estimated.
%-- %
%-- figure~\ref{fig:pa:error} shows performance forward and backward substitution
%-- for different column unrollings exemplified on one core of the hsw1 system.
%-- values are normalized to the duration required for processing one nonzero element of $l$.
%-- blue and red bars show the best and worst case predictions of the ecm model for
%-- the pardiso code,
%-- established in section~\ref{sec:pm}.
%-- %
%-- especially with panel size $s=1$, where $1$-way unrolling is used, the model
%-- error is very high in the range of about $50$\,\%.
%-- with increasing panel size and hence usage of $2$- and $8$-way unrolling the
%-- model error decreases.
%-- with panel size $s=80$ a model error of only $10$\,\% is observed.
%-- %
%-- we attribute the deviations to several inaccuracies and simplified assumptions.
%-- %
%-- first as we have seen modeling the daxpy-kernel with indirect addressing the
%-- model underestimates the performance, where it seems that overlap in the memory
%-- hierarchy is happening.
%-- with an increasing unrolling factor the impact of the indirect addressing of $r$
%-- would be decreased, hence reducing the impact of this inaccuracy.
%-- %
%-- furthermore for modeling we only include the core loops for forward and
%-- backward substitution.
%-- % das wÃ¼rde aber nur erklÃ¤ren, wenn das modell
%-- % langsammer wÃ¤re als die messung.
%-- this is clearly a point where further investigation is necessary in order to
%-- achieve better model predictions.
%-- 
%-- \subsubsection*{ecm mr}
%-- in section~\ref{sec:pm:bc} we gathered the minimal operations necessary for each
%-- unrolling during sparse triangular solve.
%-- from this we implemented a version with this minimal requirements (mr), which
%-- should be faster than pardiso as shown in figure~\ref{??}.
%-- the results in figure~\ref{fig:p:single-core} show that this is not the case as
%-- in fact mr is only as fast as pardiso.
%-- %
%-- an analysis of the generated assembly code from intrinsics for $8$-way unrolling
%-- revealed that the compiler was not able to efficiently address the eight
%-- collumns of $l$.
%-- for each column an index was used which was loaded from the stack during each
%-- access.
%-- the loads and stores saved we actually saved through row unrolling and
%-- vectorization were just replaced with spills. 
%-- as we were not able to change the compilers addressing scheme we implemented a
%-- $8$-way unrolled forward substitution with inline assembly.

% %
% the measured performance is in all cases around $20$\,\% better than the model
% prediction.
% the reason for this inaccuracy of the model is unclear and still under
% investigation.
% %
% also shown is the best case model (green).
% comparing the bars of the best case model to the ones from the ecm model of the
% original implementation reveals that most of the runtime reduction stems from a
% reduction of the time spend with loads and stores.
% %
% blue bars and values show the measurement of an optimized implementation
% according to the assumptions from section~\ref{sec:pm:bc}.
% despite intrinsics were used in order to manually vectorize the code this
% implementation does not come near the best case prediction.
% one reason is that for the $8$-way unrolling the compiler generates register
% spills, which increase the incore execution time, i.\,e.\ the number of loads
% and stores.
% therefore we reverted to an $4$-way unrolling for all optimized
% implementations (except for knights landing), which does not exhibit this
% problem, but can only handle four nonzeros per right hand side update.
% also this leads to a non-optimal situation regarding loads and stores. 
% a manual implementation in assembly seems inevitable in order to verify the best
% case model.
% 


figure~\ref{fig:p:single-core} shows performance measurements and ecm model
predictions (red bars) for all four architectures.
for knights landing predictions from the roofline model are used.
also for ivy bridge and broadwell the best case assumptions predict a
performance (green bars) which the optimized implementations (blue bars) are by
far not able to reach.

performance results for lapl1 and lapl2 matrices are shown in
figure~\ref{fig:p:pardiso}
as already pointed out the number of independent parallel parts the
factorization produces is always a power of two.
for non-power of two thread counts this results in load imbalance and decreases
performance, why they are omitted in the graphs.
% for full reference these measurements are also shown except for knights landing.
%
% the graphs show also the predicted performance through the roofline model.
% as here only the number of bytes transfered between core and memory are relevant
% the prediction depends only on the panel size. 
%
for predictions in the parallel case the fraction of nonzeros in $l$ which are
part of the separator and which can be updated in parallel is taken into
account.
this fractions depend on the input matrix and the number of used threads and
were directly extracted after factorization for each matrix.
%
for the evaluated matrices the lowest serial fraction is reached with four or
eight threads, were a performance maximum can be observed.

especially on systems with more than eight cores, like knights landing with
$64$~cores, using more cores only decreases performance.

usage of high bandwidth memory (hbm) on knights landing for all tested matrices
has no impact on performance.
despite its bandwidth scales (nearly) linearly with the number of cores its
single core bandwidth is not significantly different to the one delivered from
main memory.
only with higher core counts the full hbm bandwidth could be utilized.
with $16$, $32$, and $64$ threads the serial fraction of the lapl1 matrices
is already at around $60$\,\%, $70$\,\%, $80$\,\%, respectively.
hence, performance is dominated by the single core bandwidth.
%
however the optimized implementation with intrinsics achieves most of the time
a significantly higher performance.
in contrast to the compiler generated code from the original implementation we
use the avx512 gather and scatter instructions and implement peeling loops as
single masked simd instructions.

% \begin{figure*}[tp]%
%   \centering%
%   \perfimages{emmy}
%   \perfimages{hasep1}
%   \perfimages{broadep2}
%   \perfimages{summitridge1}
%   \perfimages{sxace}
%   \caption{xxx}
%   \label{fig:p:pardiso}%
% \end{figure*}

\begin{figure*}
%\centering
\footnotesize
%
%\input{perf-table-matrices.tex}
%
\label{fig:p:pardiso}
%
\caption{performance of benchmark matrices on all systems from the test bed for
panel size $s = 80$. please note the different scaling on the
y-axis for the sx system.}
\end{figure*}

% \begin{figure*}[tp]%
%   \centering%
%   \pimages{lapl1}{n-40-b-4}
%   \pimages{lapl2}{n-70-b-1}
%   \pimages{feti1}{mat_kii_sd22_size750141_load2_newton1}
%   \caption{performance on ivb, hsw1, hsw2, bdw, knl, zen, sx, for matrices with $\panelsize = 80$.}
%   \label{fig:p:pardiso}%
% \end{figure*}
% 
% \begin{figure*}[tp]%
%   \centering%
%   \pimages{omen1}{omen-rgf-tc2.5-lc160}
%   \pimages{omen2}{omen-rgf-tc3.5}
%   \pimages{omen3}{omen-rgf-tc4.5}
%   \caption{performance on ivb, hsw1, hsw2, bdw, knl, zen, sx, for matrices with $\panelsize = 80$
% (continued).}
%   \label{fig:p:pardiso}%
% \end{figure*}

% \begin{figure*}[tbh]
%   \centering
%   \subfloat[ivy bridge]{%
%     \includegraphics[width=0.43\textwidth,clip=true]{images/perf/perfplivbemmy-bars}
%     \label{fig:p:pardiso:laplace:ivb}
%   } \,
%   \subfloat[haswell]{%
%     \includegraphics[width=0.43\textwidth,clip=true]{images/perf/perfplhswhasep1-bars}
%     \label{fig:p:pardiso:laplace:hsw}
%   } \,
%   \subfloat[broadwell]{%
%     \includegraphics[width=0.43\textwidth,clip=true]{images/perf/perfplbdwbroadep2-bars}
%     \label{fig:p:pardiso:laplace:bdw}
%   } \,
%   \subfloat[knigths landing]{%
%     \includegraphics[width=0.43\textwidth,clip=true]{images/perf/perfplknlknightmare-bars}
%     \label{fig:p:pardiso:laplace:knl}
%   } \,
%   \caption{performance of the forward/backward substitution in pardiso
%     for the lapl-1 and lapl-2 matrix with different panel sizes $\panelsize$.  
%     group of bars show the performance for $1$, $2$, $4$, $8$, $16$, $32$, and $64$
%     threads, where available.
%     as red horizontal bars performance prediction of the ecm model is shown, when only $1$-, $2$-, or $8$-way loop unrolling over
%     columns with panel sizes $\panelsize=1$, $2$, and $80$ are used, respectively.
%     blue horiontal bars indicate performance achieved with an optimized implementation.
%     for knights landing ecm model predictions are not available.
% }
%   \label{fig:p:pardiso}
% \end{figure*}

% \begin{comment}
% \section{sparse blas/intel inspector executor}
% 
% moreover we also implemented solver using intel math kernel library
% inspector-executor sparse blas routines
% \cite{https://software.intel.com/en-us/articles/intel-math-kernel-library-inspector-executor-sparse-blas-routines}.
% we used factors computed by pardiso and implemented solve.
% intel has it's own optimized data type for sparse matrices,
% \texttt{sparse\_matrix\_t}. matrix data can be loaded from csr/csc format and then
% the matrix can be analyzed and optimized to achieve better efficiency.
% after this initialization we used triangular solve from mkl, function
% \texttt{mkl\_sparse\_d\_trsv()}, to compute forward and backward substitution.
% even though there is threaded implementation of \texttt{mkl\_sparse\_d\_trsv()}, we
% couldn't see any speedup when using more than one thread. it seems that the
% function is not able to exploit parallelism for our test matrices.
% \end{comment}

%-- \section{comparison solvers}
%-- 
%-- \todo{evtl. raus?}
%-- 
%-- for comparison we used cholmod, 
%-- mumps~\cite{amestoy-2000,amestoy-2001,amestoy-2006} and
%-- pardiso~\cite{schenk-2004,kuzmin-2013} solvers.
%-- for cholmod and mumps we set metis reordering, so all solvers have about the
%-- same factors. 
%-- we measured duration of the forward/backward substitution for our benchmark
%-- matrices.
%-- the results for the ivy bridge system are shown in figure~\ref{fig:solvers}.
%-- the matrices as well as the hardware system are described in
%-- section~\ref{sec:tb}.
%-- %
%-- \begin{figure*}[tp]
%--   \centering
%--     \includegraphics[width=0.95\textwidth,clip=true]{images/solvercomparison}
%--   \caption{duration of the solve step for all benchmark matrices for different
%-- sparse direct solvers on the ivb system.}
%--   \label{fig:solvers}
%-- \end{figure*}

\section{conclusion}
\label{sec:conclusion}

\todo{only some ideas, need rework}
modelled sparse fw/bw subst via ecm model considering caching effects for
different column lengths and cache sizes.
established a best case model with vectorization and blocking for maximizing
data reuse.
implemented a version according to this considerations which was far of
epectations, it achieved around the original performance.
reasons are inefficencies regarding loads and stores introduced by the compiler,
e.\,g.\ through register spills.
manual implementation in assembly to verify the best case model necessary.
reason is the non-support of gather/scatter instructions, which makes manually
implementing this tedious. 
although with avx2 gather was introduced, which would be enough for backward substitution, 
scatter for the forward substitution is missing.
only the xeon phi line has fully support for all flavors of gather and scatter
operations.
on our knights landing the intrinsic implementation improved performance most of
the time significantly.
\end{comment}


\end{comment}
\begin{comment}
\chapter{\todol{book chapter: State-of-The-Art Sparse Direct Solvers}}

\section{Introduction}
\label{sec:intro}
Solving large sparse linear systems is at the heart of many application
problems arising from computational science and engineering applications.
Advances in combinatorial
methods in combination with modern computer architectures have massively
influenced the design of state-of-the-art direct solvers
that are feasible for solving  larger systems efficiently
in a computational environment with rapidly increasing memory resources
and cores. Among these advances are 
novel combinatorial algorithms for improving diagonal dominance which
pave the way to a static pivoting approach, thus improving the 
efficiency of the factorization
phase dramatically. Besides, partitioning and reordering the system
such that a high level of concurrency is achieved, the objective is to 
simultaneously achieve the reduction of fill-in and the parallel concurrency.
While these achievements already significantly improve the factorization
phase, modern computer architectures require one to compute as many operations
as possible in the cache of the CPU. This in turn can be achieved when
dense subblocks that show up during the factorization can be grouped
together into dense submatrices which are handled
by multithreaded and cache-optimized 
dense matrix kernels using level-3 BLAS and LAPACK
\cite{AndBBDDDGHMOS95}.

This chapter will review some of the basic technologies together
with the latest developments for sparse direct solution methods that have led to
state-of-the-art $LU$ decomposition methods.
The paper is organized as follows. In Section \ref{sec:mwm}
we will start with maximum weighted matchings which is one of the key
tools in combinatorial optimization to dramatically improve the diagonal dominance
of the underlying system.
Next, Section \ref{sec:reordering} will review multilevel nested dissection
as a combinatorial method to reorder a system symmetrically
such that fill-in and parallelization can are improved simultaneously, once
pivoting can be more or less ignored.
After that, we will review established graph-theoretical approaches
in Section \ref{sec:lu}, in particular the elimination tree, from which
most of the properties of the $LU$ factorization can be concluded. Among
these properties is the prediction of dense submatrices in the
factorization. In this way several subsequent
columns of the factors $L$ and $U^T$ are collected in a single dense block. 
This is the basis for the use of dense matrix kernels using optimized
level-3 BLAS as well to exploit fast computation using the cache hierarchy which 
is discussed in Section~\ref{sec:parallel}.
Finally, we show in Section~\ref{sec:appl} how the ongoing developments in parallel sparse direct solution methods have advanced integrated circuit simulations.
We assume that the reader
 is familiar with some elementary knowledge from
graph theory, see; e.g., \cite{DufER86,GeoL81} and some simple
computational algorithms based on graphs \cite{AhoHU83}.

\section{Maximum weight matching}
\label{sec:mwm}
In modern sparse elimination methods the key to success 
is ability to work with efficient data structures and their underlying
numerical templates. If we can increase the size of the diagonal entries
as much as possible in advance, pivoting during Gaussian elimination can often 
be bypassed and we may work with static data structures and
the numerical method will be significantly accelerated. 
A popular method to achieve this goal is the
maximum weight matching method~\cite{DufK99S,olschowka:1996} 
which permutes (e.g.) the rows of a given
nonsingular matrix $A\in\R^{n,n}$ by a permutation matrix $\Pi\in\R^{n,n}$ 
such that $\Pi^TA$
has a \emph{non-zero diagonal}. Moreover, it maximizes 
the product of the absolute diagonal values  and yields diagonal
scaling matrices $D_r, D_c\in\R^{n,n}$ such that $\tilde A=\Pi^TD_rAD_c$ satisfies
$|\tilde a_{ij}|\leqslant 1$ and $|\tilde a_{ii}|=1$ for all $i,j=1,\dots,n$.
The original idea on which these nonsymmetric permutations and scalings are
based is to find a \emph{maximum weighted matching} of a
\emph{bipartite graphs}. Finding a maximum weighted matching is a well
known assignment problem in operation research and combinatorial
analysis.
\begin{definition}\label{def:bipartite}
A graph $G=(V,E)$ with vertices $V$ and edges $E\subset V^2$ 
is called \emph{bipartite} 
if $V$ can be partitioned into two sets  $V_r$ and  $V_c$, such that no edge
$e=(v_1,v_2) \in E$ has both ends $v_1,v_2$ in $V_r$ or both ends $v_1,v_2$ 
in $V_c$. In this case we denote $G$ by $G_b=(V_r,V_c,E)$.
\end{definition}
%\begin{definition}
%For $A\in\R^{n,n}$ its associated graph is given $G(A)=(V,E)$,
%where $V=\{1,\dots,n\}$ and $E=\{(i,j)|\; a_{ij}\not=0\}$.
%\end{definition}
\begin{definition}\label{def:bipartite-graph}
Given a matrix $A$, then we can associate with it a canonical
bipartite graph $G_b(A)=(V_r,V_c,E)$ by assigning the 
labels of $V_r=\{r_1,\dots,r_n\}$ 
with the row indices of $A$ and 
$V_c=\{c_1,\dots,c_n\}$ being labeled by the column indices.
In this case $E$ is defined via $E=\{(r_i,c_j)|\; a_{ij}\not=0\}$.
\end{definition}
For the bipartite graph $G_b(A)$ we see immediately that 
If $a_{ij}\not=0$, 
then we have that $r_i \in V_r$ from the row set
is connected by an edge $(r_i,c_j) \in E$ to the column $c_j \in V_c$,
but neither rows are connected with each other nor do the columns have
inter connections.
\begin{definition}\label{def:matching}
A \emph{matching} $\cM$ 
of a given graph $G= (V,E)$ is a subset of edges
$e\in E$ such that no two of which share the same vertex. 
\end{definition}
If $\cM$ is a
matching of a bipartite graph $G_b(A)$, then each edge $e=(r_i,c_j) \in \cM$ 
corresponds to a row $i$ and a column $j$ and there exists no other edge 
$\hat e=(r_k,c_l) \in \cM$ 
that has the same vertices, neither $r_k=r_i$ nor $c_l=c_j$. 
\begin{definition}\label{def:maxmatching}
A matching $\cM$ of $G=(V,E)$ is called
\emph{maximal}, if no other edge from $E$ can be added to $\cM$.
\end{definition}
If, for an $n \times n$ matrix $A$ a \emph{matching} $\cM$ of $G_b(A)$ with
maximum cardinality $n$ is found, then by definition the edges 
must be $(i_1,1),\dots,(i_n,n)$ with $i_1,\dots,i_n$ being the 
numbers $1,\dots,n$ in a suitable order and therefore we obtain
$a_{i_1,1}\not=0$, \dots
$a_{i_n,n}\not=0$. In this case 
we have established that the
matrix $A$ is at least structurally nonsingular and we can use a 
row permutation matrix $\Pi^T$ associated with row ordering $i_1,\dots,i_n$ 
to place a nonzero entry on each diagonal location of $\Pi^TA$.
\begin{definition}\label{def:perfect-matching}
A \emph{perfect matching} is a maximal matching with cardinality $n$.
\end{definition}
It can be shown that for a structurally nonsingular matrix $A$ there always
exists a perfect matching $\cM$.
%\begin{theorem}\label{hopcraft}
%When $A\in\R^{n,n}$ is structurally nonsingular, then 
%there always exists  a perfect
%matching for $G_b(A) = (V_r, V_c, E)$. The perfect matching $\cM$ defines
%an $n \times n$ permutation matrix with
%\[
%\Pi^T = (p_{ij}) =  \left\{ 
%\begin{array}{cc} 
%p_{ij} = 1 & e_{ij} \in M \\
%p_{ij} = 0 & e_{ij} \not \in M  
%                          \end{array} \right.
%\]
%\end{theorem}
\begin{example}{Perfect Matching}\label{exm:perfect_matching}
In Figure \ref{fig:unsym_perm}, the set of edges $\cM= \{(1,2), (2,4),
(3,5), (4,1), (5,3), (6,6) \}$ represents a perfect maximum matching
of the bipartite graph $G_b(A)$.
\end{example}
\begin{figure}
% \sidecaption
\begin{minipage}{.33\textwidth}
 \begin{center}
 Original Matrix $A$
    $\left(
%
        \begin{array}{cccccc}
         % 1 & 3 & \nl &  2 & \nl & \nl \\
         1 & 3 & \nl & \nl & \nl & \nl \\
          3  & \nl & \nl & 4 & \nl & 1 \\
        \nl & \nl & \nl & \nl &  3  & \nl \\
        % 2 &  4  & \nl & \nl & 1 & \nl \\
        2 &  \nl  & \nl & \nl & 1 & \nl \\
        \nl & \nl &  3  & 1 & \nl & \nl \\
        \nl & 1 & \nl & \nl & \nl &  2
        \end{array}
    \right) $ 
 \end{center}
\end{minipage}
\begin{minipage}{.32\textwidth}
 \begin{center}
$G_b(A): \;$ 
\includegraphics[width=0.43\textwidth]{figures/matching1} 
%
%\medskip
%
\hspace{0.5cm}$\cM: \;$ 
\includegraphics[width=0.43\textwidth]{figures/matching2} 
 \end{center}
\end{minipage}
\begin{minipage}{.32\textwidth}
  \begin{center}
Reordered Matrix $\Pi^TA$

    $\left(
        \begin{array}{cccccc}
        % 2 &  4  & \nl & \nl & 1 & \nl \\
        2 &  \nl  & \nl & \nl & 1 & \nl \\
        % 1 & 3 & \nl &  2 & \nl & \nl \\
         1 & 3 & \nl &  \nl & \nl & \nl \\
        \nl & \nl &  3  & 1 & \nl & \nl \\
         3  & \nl & \nl & 4 & \nl & 1 \\
        \nl & \nl & \nl & \nl &  3  & \nl \\
        \nl & 1 & \nl & \nl & \nl &  2
        \end{array}
    \right) $ 
% \medskip
 \end{center}  
\end{minipage}
    \caption{Perfect matching. Left side: original
      matrix $A$. Middle: bipartite representation $G_b(A) = (V_r, V_c, E)$
      of the matrix $A$ and perfect matching $\cM$. Right side: permuted matrix
      $\Pi^TA$.}
    \label{fig:unsym_perm}
\end{figure}

The most efficient combinatorial methods for finding maximum matchings
in bipartite graphs make use of an \emph{augmenting path}. We will
introduce some graph terminology for the
 construction of perfect
matchings. 
\begin{definition}\label{def:path}
If an edge $e=(u,v)$ in a graph $G=(V,E)$
joins a vertices $u,v\in V$, then we denote it as $uv$. 
A path then consists of edges $u_1u_2,u_2u_3,u_3u_4 \ldots,u_{k-1}u_k$, where 
each $(u_i,u_{i+1})\in E$, $i=1,\dots,k-1$.
\end{definition}
If $G_b=(V_r,V_c,E)$ is a bipartite graph, then by definition of a path, 
any path is alternating between the vertices of $V_r$ and $V_c$, e.g.,
paths in $G_b$ could be such as $r_1c_2,c_2r_3,r_3c_4,\dots$.
\begin{definition}\label{def:various-paths}
Given a graph $G=(V,E)$, a 
vertex is called \emph{free} if it is not
incident to any other edge in a matching $\cM$ of $G$. 
An \emph{alternating path} relative to a matching $\cM$ is a path 
$P = u_1u_2,u_2u_3, \ldots,u_{s-1}u_s$ where its edges are alternating 
between $E \setminus \cM$ and $\cM$. An
\emph{augmenting path} relative to a matching $\cM$ is an alternating
path of odd length and both of it vertex endpoints are free. 
\end{definition}
\begin{example}{Augmenting Path}\label{exm:augmenting_path}
Consider Figure \ref{fig:unsym_perm}.
To better distinguish between row and column vertices we use
$\fbox{$1$},\fbox{$2$},\dots,\fbox{$6$}$ for the rows and \circn{1},\circn{2},\dots,\circn{6} for the
columns.
A non-perfect but maximal matching is given by
$M= \{(\fbox{$4$},$\circn{5}$), (\fbox{$1$},$\circn{1}$), (\fbox{$6$},$\circn{2}$), (\fbox{$2$},$\circn{6}$), (\fbox{$5$},$\circn{4}$) \}$. 
We can easily see that an augmenting path 
alternating between rows and columns is given by \fbox{$3$}\circn{5} , \circn{5}\fbox{$4$} , \fbox{$4$}\circn{1} , \circn{1}\fbox{$1$} , \fbox{$1$}\circn{2} , \circn{2}\fbox{$6$} , \fbox{$6$}\circn{6} , \circn{6}\fbox{$2$} , \fbox{$2$}\circn{4} , \circn{4}\fbox{$5$} , \fbox{$5$}\circn{3}. Both endpoints \fbox{$3$} and \circn{3}
of this augmenting path are free.
\end{example}

In a bipartite
graph $G_b= (V_r, V_c, E)$ one vertex endpoint of any
augmenting path must be in $V_r$ whereas the other one must be in $V_c$. 
The symmetric
difference, $A \oplus B$ of two edge sets $A$, $B$ is defined to be $(A
\setminus B) \cup (B \setminus A)$.

Using these definitions and notations,
the following theorem \cite{Berge} gives a
constructive algorithm for finding perfect matchings in bipartite
graphs.

\begin{theorem}\label{theo:Berge}
If $\cM$ is non-maximum matching of a bipartite graph $G_b= (V_r, V_c,E)$, 
then there exists an augmenting path $P$ relative to $\cM$ such that
 $P=\tilde{\cM} \oplus \cM$ and $\tilde{\cM}$
is a matching with cardinality $|\cM|+1$.
\end{theorem}
According to this theorem, a combinatorial method of finding perfect
matching in a bipartite graph is to seek augmenting paths. 

The
perfect matching as discussed so far only takes the nonzero structure
of the matrix into account. 
For their use as static pivoting methods prior to the $LU$ decomposition
one requires in addition to
maximize the absolute value of the product of the diagonal entries. 
This is referred to as
\emph{maximum weighted matching}. In this case a permutation
$\pi$ has to be found, which maximizes
\begin{equation}
  \prod_{i=1}^n |a_{\pi(i)i}|. \label{eq:1}
\end{equation}
The maximization of this product is transferred into a minimization of a sum as follows. We define a matrix $C = (c_{ij})$ via
\[
  c_{ij} = 
  \begin{cases}
    \log a_i - \log |a_{ij}| & a_{ij} \neq 0 \\
    \infty                     & \text{otherwise},
  \end{cases}
\]
where $a_i = \max_j |a_{ij}|$  is the maximum element in row $i$ of
matrix $A$. A permutation $\pi$ which minimizes the sum 
\[
  \label{eq:4}
  \sum_{i=1}^n c_{\pi(i)i} 
\]
also maximizes the product~(\ref{eq:1}). The minimization problem is
known as linear-sum assignment problem or bipartite weighted matching
problem in combinatorial optimization.  The problem is solved by a
sparse variant of the Hungarian method. The complexity is
 $\mathcal{O}(n \tau \log n )$ for
sparse matrices with $\tau$ entries. For matrices, whose associated
graph fulfill special requirements, this bound can be reduced further
to $\mathcal{O}(n^\alpha (\tau + n \log n))$ with $\alpha < 1$.  All graphs
arising from finite-difference or finite element discretizations meet
the conditions~\cite{gupta:99}. As before, we finally get a perfect
matching which in turn defines a nonsymmetric permutation.

When solving the assignment problem, two dual vectors $u = (u_i)$ and
$v = (v_i)$ are computed which satisfy
\begin{align}
  u_i + v_j & = c_{ij} \qquad  (i,j) \in \cM, \label{eq:11} \\
  u_i + v_j & \leq c_{ij} \qquad \text{otherwise}. \label{eq:12}
\end{align}
Using the exponential function these vectors can be used to scale the 
initial matrix. To do so define
two diagonal matrices $D_r$ and $D_c$ through
\begin{align}
  D_r & = \text{diag}(d_1^r,d_2^r,\dots,d_n^r), \qquad d_i^r = \exp(u_i),\\ 
  D_c & = \text{diag}(d_1^c,d_2^c,\dots,d_n^c), \qquad d_j^c = \exp(v_j)/a_j.
\end{align}
Using equations (\ref{eq:11}) and (\ref{eq:12}) and the definition of $C$, 
it immediately follows that $\tilde A = \Pi^T D_r A D_c$ satisfies
\begin{align}
  |\tilde a_{ii}| & = 1, \label{eq:13}\\
  |\tilde a_{ij}| & \le 1. \label{eq:14}
\end{align}
The permuted and scaled system $\tilde A$ has been observed to
have significantly better numerical properties when being used
for direct methods or for preconditioned iterative methods, cf. e.g. 
\cite{benzi:2000:phi,DufK99S}. Olschowka and
Neumaier~\cite{olschowka:1996} introduced these scalings and
permutation for reducing pivoting in Gaussian elimination of full
matrices. The first implementation for sparse matrix problems was
introduced by Duff and Koster~\cite{DufK99S}.  For symmetric
matrices $|A|$, these nonsymmetric matchings can be converted
to a symmetric permutation $P$ and a symmetric scaling $D_s=(D_rD_c)^{1/2}$
such that $P^TD_sAD_sP$ consists mostly of diagonal blocks of size $1\times 1$
and $2\times 2$ satisfying a similar condition as (\ref{eq:13}) and (\ref{eq:14}),
where in practice it rarely happens that  $1\times 1$ blocks are identical 
to $0$~\cite{dupr:04a}.
Recently, successful parallel approaches to compute maximum weighted matchings have
been proposed~\cite{LanPM11,LanAM14}.

\begin{example}{Maximum Weight Matching}\label{exm:west0479-match}
To conclude this section we demonstrate the effectiveness of maximum weight matchings using a simple sample matrix ``west0479'' from the SuiteSparse Matrix Collection.
The matrix can also directly be loaded in \ml{} using \texttt{load west0479}.
In Figure \ref{fig:mwm} we display the matrix before and after applying maximum
weighted matchings. To illustrate the improved diagonal dominance we further
compute $r_i=|a_{ii}|/\sum_{j=1}^n|a_{ij}|$ for each row of $A$ and $\tilde A=\Pi^TD_rAD_s$, $i=1,\dots,n$. $r_i$ can be read as relative diagonal dominance of row $i$ 
and yields a number between $0$ and $1$. Moreover, whenever $r_i>\frac12$, the row
is strictly diagonal dominant, i.e., $|a_{ii}|>\sum_{j:j\not=i}|a_{ij}|$.
In Figure \ref{fig:mwm} we display for both matrices $r_i$ by sorting its values
in increasing order and taking $\frac12$ as reference line. We can see the
dramatic impact of maximum weighted matchings in improving the diagonal dominance
of the given matrix and thus paving the way to a static pivoting approach
in incomplete or complete $LU$ decomposition methods.
\end{example}
\begin{figure}
% \sidecaption
\begin{minipage}{.45\textwidth}
 \begin{center}
% Original Matrix $A$
%
\includegraphics[width=0.8\textwidth]{figures/west0479} 
 \end{center}
\end{minipage}
~
\begin{minipage}{.45\textwidth}
  \begin{center}
%Reordered and Rescaled Matrix 
%
\includegraphics[width=0.8\textwidth]{figures/west0479-match} 
 \end{center}  
\end{minipage}
    \caption{Maximum weight matching. Left side: original
      matrix $A$. Right side: permuted and rescaled matrix
      $\tilde A=\Pi^TD_rAD_c$.}
    \label{fig:mwm}
\end{figure}
\begin{figure}
% \sidecaption
\begin{minipage}{.48\textwidth}
 \begin{center}
% Original Matrix $A$
%
\includegraphics[width=0.95\textwidth,height=0.5\textwidth]{figures/west0479-dd} 
 \end{center}
\end{minipage}
~
\begin{minipage}{.48\textwidth}
  \begin{center}
%Reordered and Rescaled Matrix 
%
\includegraphics[width=0.95\textwidth,height=0.5\textwidth]{figures/west0479-match-dd} 
 \end{center}  
\end{minipage}
    \caption{Diagonal dominance. Left side: $r_i$ for $A$. Right side: $r_i$  $\tilde A=\Pi^TD_rAD_c$.}
    \label{fig:mwm-dd}
\end{figure}


\section{Symbolic symmetric reordering techniques} 
\label{sec:reordering}
When dealing with large sparse matrices a crucial factor that determines
the computation time is the amount of fill that is produced during the
factorization of the underlying matrix. To reduce the complexity there
exist many mainly symmetric reordering techniques that attempt to reduce
the fill-in heuristically. Here we will demonstrate only one of these
methods, the so-called nested dissection method. The main reason for selecting
this method is that it can be easily used for parallel computations.

\subsection{Multilevel nested dissection}
\label{subsec:mnd}
Recursive multilevel nested dissection methods for direct
decomposition methods were first introduced in the context of
multiprocessing. If parallel direct methods are used to solve a sparse
system of equations, then a graph partitioning algorithm can be used
to compute a fill reducing ordering that leads to a high degree of
concurrency in the factorization phase. 
\begin{definition}\label{def:matrix-graph}
For a matrix $A\in\R^{n,n}$ we
 define the associated (directed) graph $G_d(A)=(V,E)$, where 
$V=\{1,\dots,n\}$ and the set of edges 
$E=\left\{(i,j)|\, a_{ij}\not=0\right\}$.
The (undirected) graph is given by  $G_d(|A|+|A|^T)$ and is denoted
simply by $G(A)$.
\end{definition}
In graph terminology for a sparse matrix $A$ we simply have a directed
edge $(i,j)$ for any nonzero entry $a_{ij}$ in $G_d(A)$ whereas the 
orientation of the edge is ignored in $G(A)$.

The research on graph-partitioning
methods in the mid-nineties has resulted in high-quality software
packages, e.g. \metis {} \cite{karypis:98}.
These methods often compute orderings that on the one hand lead to small fill-in 
for (incomplete) factorization methods while on the other hand they
provide a high level of concurrency.
We will briefly review the main idea of multilevel nested dissection in
terms of graph-partitioning.
\begin{definition}\label{def:partitioning-and-separator}
Let $A\in\R^{n,n}$
and consider its graph $G(A)=(V,E)$. 
A \emph{$k$-way graph partitioning} consists of 
partitioning $V$ into $k$ disjoint subsets
$V_1, V_2, \ldots, V_k$ such that $V_i \cap V_j = \emptyset$ for $i
\ne j$  $\cup_i V_i=V$.
The subset $E_s = E\cap \bigcup_{i\not=j} (V_i\times V_j)$ is called 
\emph{edge separator}.
\end{definition}
Typically we want a $k$-way partitioning to be balanced, i.e., 
each $V_i$ should satisfy $|V_i|\approx n/k$. The edge separator $E_s$
refers to the edges that have to be taken away from the graph
in order to have $k$ separate
subgraphs associated with $V_1,\dots,V_k$ and the number of elements of
$E_s$ is usually referred to as edge-cut. 

\begin{definition}\label{def:vertex-separator}
Given $A\in\R^{n,n}$,
    a \emph{vertex separator} $V_s$ of $G(A)= (V,E)$ is a
    set of vertices such that there exists a $k$-way partitioning 
    $V_1, V_2, \ldots, V_k$ of $V \setminus V_s$ having no edge
    $e\in V_i\times V_j$ for $i\ne j$. 
\end{definition}
A useful vertex separator $V_s$ should not only separate $G(A)$ into
$k$ independent subgraphs associated with $V_1,\dots,V_k$, it is 
intended that the numbers of edges 
$\cup_{i=1}^{k} |\{ e_{is} \in V_i, s \in V_s\}| $ is also small.



Nested dissection recursively splits a graph $G(A)= (V,E)$ into almost
equal parts by constructing a vertex separator $V_s$ 
until the desired number $k$ 
of partitionings are obtained. If $k$ is a power of $2$, then a natural
way of obtaining a vertex separator
is to first obtain a $2$-way partitioning of the graph, a so called
\emph{graph bisection} with its associated edge separator $E_s$.
After that a vertex separator $V_s$ is computed from $E_s$, which
gives a $2$-way partitioning $V_1,V_2$ of $V\setminus V_s$.
This process is then repeated separately
for the subgraphs associated with $V_1,V_2$ until eventually a
$k=2^l$-way partitioning is obtained. For the reordering of the
underlying matrix $A$, the vertices associated with $V_1$ are taken first
followed by $V_2$ and $V_s$. This reordering is repeated similarly during
repeated bisection of each $V_i$. In general, vertex separators
of small size result in low fill-in.

\begin{example}{Vertex Separators}\label{exm:vsep}
To illustrate vertex separators, we consider the reordered matrix $\Pi^TA$
from Figure \ref{fig:unsym_perm} after a  matching is applied.
In Figure \ref{fig:matrixvertex} we display its graph $G(\Pi^T A)$ ignoring
the orientation of the edges. A 
$2$-way partitioning is obtained with $V_1 = \{3,5\}$, $V_2 = \{2,6\}$ and
a vertex separator $V_s = \{1,4\}$. The associated reordering
refers to taking the rows and the columns of $\Pi^T A$ in the order
$3,5,2,6,1,4$.
\end{example}
\begin{figure}%[t]
%\sidecaption
%\fbox
{
\begin{minipage}{7.0cm}
%\fbox
{
\begin{minipage}{.6\textwidth}
\includegraphics[width=\textwidth]{figures/matrixvertex}
% \caption{This is the second picture.}
% \label{fig:4}
\end{minipage}
}
\hfil
%\fbox
{
\begin{minipage}{.3\textwidth}
    $\left(
        \begin{array}{cc|cc|cc}
        3   & \nl & \nl & \nl & \nl   &  1  \\
        \nl & 3   &  \nl& \nl & \nl & \nl   \\ \hline
        \nl &  \nl& 3   & \nl &  1 & \nl \\
        \nl & \nl &  1 &  2  & \nl   & \nl \\ \hline
        \nl  & 1  & \nl & \nl & 2   & \nl   \\
        \nl   & \nl & \nl & 1   & 3 & 4   
        \end{array}
    \right)$
\end{minipage}
}
\end{minipage}
}
\caption{A $2$-way partition with vertex separator $V_s=\{1,4\}$
and the associated reordered matrix placing the two rows and columns associated 
with $V_s$ to the end.}\label{fig:matrixvertex}
\end{figure}

Since a naive approach to compute a recursive graph bisection is 
typically computationally expensive,
combinatorial \emph{multilevel graph bisection} has been used to
accelerate the process. The basic structure is simple. The multilevel approach
consists of three phases: at first there is a \emph{coarsening phase}
which compresses the given graph successively
level by level by about half of its size. When the coarsest graph with about
a few hundred vertices is reached, the second phase,  namely the so-called
\emph{bisection} is applied. This is a high quality partitioning algorithm.
After that, during the \emph{uncoarsening phase}, the given
bisection is successively refined as it is prolongated towards the original
graph. 

\subsubsection*{Coarsening Phase}
The initial graph $G_0=(V_0,E_0)=G(A)$ of $A\in\R^{n,n}$ is transformed during
the coarsening phase
into a sequence of graphs $G_1, G_2, \ldots, G_m$ of decreasing size 
such that $|V_0|\gg|V_1|\gg|V_2|\gg\cdots\gg|V_m|$. 
Given the graph $G_i=(V_i,E_i)$, the
next coarser graph $G_{i+1}$ is  obtained from $G_i$ by collapsing adjacent
vertices. This can be done e.g. by using a maximal matching $\cM_i$ of $G_i$ (cf. Definitions \ref{def:matching} and \ref{def:maxmatching}).
Using $\cM_i$, the next  coarser graph $G_{i+1}$ is 
constructed from $G_i$ collapsing the vertices 
being matched into multinodes, i.e., the elements of  $\cM_i$ together with the
unmatched vertices of $G_i$ become the new vertices $V_{i+1}$ of $G_{i+1}$. 
The new edges $E_{i+1}$ are the remaining edges from $E_i$ 
connected with the collapsed vertices. 
There are various differences in the construction of maximal matchings
\cite{karypis:98,CheP08}. 
One of the most popular and efficient methods is heavy edge
matching \cite{karypis:98}.

\subsubsection*{Partitioning Phase}
At the coarsest level $m$,
a $2$-way partitioning $V_{m,1}\dot{\cup}V_{m,2}=V_m$ of $G_m=(V_m,E_m)$ is computed,
each of them containing about half of the vertices of $G_m$.
This specific partitioning of $G_m$ can be obtained by using various
algorithms such as spectral bisection \cite{fiedler:75} or
combinatorial methods based on Kernighan-Lin variants
\cite{KerL70,FidM97}. It is demonstrated in \cite{karypis:98} that 
for the coarsest graph, combinatorial
methods typically compute smaller edge-cut separators compared with
spectral bisection methods. However, since
the size of the coarsest graph $G_m$ is small (typically $|V_m|<100)$, this
step is negligible with respect to the total amount of computation time.

\subsubsection*{Uncoarsening Phase}
Suppose that at the coarsest level $m$, an edge separator $E_{m,s}$ 
of $G_m$ associated with the  $2$-way partitioning has been computed 
that has lead to a sufficient edge-cut of $G_m$ with $V_{m,1}$, $V_{m,2}$
of almost equal size.
Then $E_{m,s}$ is prolongated to $G_{m-1}$ by reversing the process of
collapsing matched vertices. This leads to an initial edge separator
$E_{m-1,s}$ for $G_{m-1}$. But since $G_{m-1}$ is finer, $E_{m-1,s}$ is 
sub-optimal and one usually decreases the edge-cut of the partitioning
by local refinement heuristics such as the
Kernighan-Lin partitioning algorithm \cite{KerL70} 
or the Fiduccia-Mattheyses method \cite{FidM97}.
Repeating this refinement procedure level-by-level we obtain a sequence
of edge separators $E_{m,s},E_{m-1,s},\dots,E_{0,s}$ and eventually and
edge separator $E_{s}=E_{0,s}$ of the initial graph $G(A)$ is obtained.
If one is seeking for a vertex separator $V_s$ of $G(A)$, then one usually 
computes $V_s$ from $E_s$ at the end.

There have been a number of methods that are used for graph partitioning,
e.g. \metis{} \cite{karypis:98}, a parallel MPI version \parmetis{} \cite{KarSK99},
or a recent multithreaded approach \mtmetis \cite{LasK13}.
Another example for a parallel partitioning algorithm is \scotch \cite{CheP08}. 

\begin{example}{Multilevel Nested Dissection}\label{exm:west0479-metis}
We will continue Example \ref{exm:west0479-match} using the matrix
$\tilde A=\Pi^TD_rAD_s$ that has been rescaled and permuted using
maximum weight matching. We illustrate in Figure \ref{fig:metis} 
how multilevel nested dissection changes the pattern $\hat A=P^T \tilde A P$,
where $P$ refers to the permutation matrix associated with the partitioning
of $G(\tilde A)$.
\end{example}
\begin{figure}
%\sidecaption
\begin{minipage}{.55\textwidth}
  \begin{center}
\includegraphics[width=0.95\textwidth]{figures/west0479-match-metis} 
 \end{center}  
\end{minipage}
    \caption{Application of multilevel
nested dissection after the matrix is already rescaled and permuted using maximum weight matching.}
    \label{fig:metis}
\end{figure}


\subsection{Other reordering methods}
One of the first methods to reorder the system was the
reverse Cuthill-McKee (\rcm)  methods \cite{cm:69,LiuS76} which attempts
to reduce the bandwidth of a given matrix. Though this algorithm is still 
attractive for sequential methods and incomplete factorization methods, its use
for direct solvers is considered as obsolete. An attractive alternative to 
nested dissection as reordering method for direct factorization methods is
the minimum degree algorithm (\mmd) \cite{Ros72,GeoL89} and its recent variants,
in particular the approximate minimum degree algorithm (\amd) \cite{AmeDD96,Dav06} 
with or without constraints. The main objective of the minimum degree algorithm
is to simulate the Gaussian elimination process symbolically by investigating
the update process $a_{ij}\to a_{ij}-a_{ik}a_{kk}^{-1}a_{kj}$ by means of graph
theory, at least in the case of the undirected graph. 
The name-giving degree refers to the number of edges connected to a vertex and 
how the graph and therefore the degrees of its vertices change during the 
factorization process.
Over the years this
has lead to an evolution of the underlying minimum degree algorithm
using the so-called \emph{external degree} for selecting vertices as pivots
and  
further techniques like \emph{incomplete degree update}, 
\emph{element absorption} and \emph{multiple elimination} 
as well as data structures based on cliques. 
For an overview see \cite{GeoL89}.
One of the most costly parts in the minimum degree algorithm  is to update
of the degrees. Instead of computing the exact external degree, in
the approximate minimum degree algorithm
\cite{AmeDD96} an approximate external degree is computed that significantly
saves time while producing comparable fill in the $LU$ decomposition.

We like to conclude this section by mentioning that if nested dissection is computed
to produce a vertex separator $V_s$ and a related $k$-way partitioning $V_1,\dots,V-k$ for the remaining vertices of $V\setminus V_s$ of $G(A)=(V,E)$ 
which allow for parallel
computations, then the entries of each $V_i$, $i,\dots,k$ could be taken in
any order. Certainly, inside $V_i$ one could use nested dissection as well, which
is the default choice in multilevel nested dissection methods. However, as soon
as the coarsest graph $G_m$ is small enough (typically about $100$ vertices),
not only the separator is computed, but in addition the remaining entries of
$G_m$ are reordered to lead to a fill-reducing ordering. In both cases, for
$G_m$ as well as $V_1,\dots,V_k$ one could alternatively use different reordering
methods such as variants of the minimum degree algorithm. Indeed, for
$G_m$ this is what the \metis software is doing. Furthermore, a reordering
method such as the constrained approximate minimum degree algorithm is also
suitable as local reordering for $V_1,\dots,V_k$ as alternative to nested 
dissection, taking into account the edges connected with $V_s$ (also referred to
as HALO structure), see e.g. \cite{PelRA00}.


\section{Sparse $LU$ Decomposition}
\label{sec:lu}
In this section we will assume that the given matrix $A\in\R^{n,n}$ is nonsingular and that it can be factorized as $A=LU$, where $L$ is a lower triangular matrix with unit diagonal and
$U$ is an upper triangular matrix. 
It is well-known \cite{GeoL81}, if $A=LU$, where $L$ and $U^\top$ are lower
triangular matrices, then in the generic case we will have
$G_d(L+U)\supset G_d(A)$, i.e., we will only get additional edges unless some
entries cancel by ``accident'' during the elimination. In the sequel
we will ignore cancellations. Throughout this section we will always assume
that the diagonal entries of $A$ are nonzero as well. We also assume that $G_d(A)$
is connected.


In the preceding sections we have argued that
maximum weight matching often leads to a rescaled and reordered matrix such that
static pivoting is likely to be enough, i.e., 
pivoting is restricted to some dense blocks inside the $LU$ factorization.
Furthermore, reordering strategies such as multilevel nested dissection have 
further symmetrically permuted the system such that the fill-in that occurs
during Gaussian elimination is acceptable and even parallel approaches could
be drawn from this reordering. Thus assuming that $A$ does not need further
reordering and a factorization $A=LU$ exists 
is a realistic scenario in what follows.




%We begin with some basic terminology. 
%For a matrix $A=\left(a_{ij}\right)_{i,j=1,\dots,n}\in\R^{n,n}$ we denote by $A_{k:l,p:q}$ the submatrix $\left(a_{ij}\right)_{i=k,\dots,l, j=p,\dots,q}$ of $A$. Here we always assume that $1\leqslant k\leqslant l\leqslant n$ and $1\leqslant p\leqslant q\leqslant n$. 
% Next we introduce some definitions from graph theory associated with a given matrix $A\in\R^{n,n}$.
%\begin{definition}
%For a matrix $A\in\R^{n,n}$ we
% define the associated graph $G(A)$ by the pair $(V,E)$, where 
%$V=\{1,\dots,n\}$ and the set of edges 
%$E=\left\{(i,j):\, a_{ij}\not=0\right\}$.
%\end{definition}
% Whenever we refer to an \emph{undirected} graph we mean that $(i,j)\in G(A)$ if and only if $(j,i)\in G(A)$. In this case we may also use $\{i,j\}$ instead of $(i,j)$.


\subsection{The Elimination Tree}
\label{subsec:etree}
% We assume that our matrix $A$ possesses an $LU$ decomposition (see e.g \cite{GolV96}). 
The basis of determining the fill-in in the triangular factors 
$L$ and $U$ as by-product of the Gaussian elimination can be characterized
as follows (see \cite{Gil94} and the references therein). 

\begin{theorem}\label{thr:gilbert}
Given $A=LU$ with the aforementioned assumptions, 
there exists an edge $(i,j)$ in $G_d(L+U)$ if and only if there exists a path
\[
ix_1, x_2x_3, \dots, x_kj
\]
in $G_d(A)$ such that $x_1,\dots,x_k<\min(i,j)$.
\end{theorem}
In other words, during Gaussian elimination we obtain a fill edge $(i,j)$ for
every path from $i$ to $j$ through vertices less than $\min(i,j)$.

\begin{example}{Fill-in}\label{exm:fill}
We will use the matrix $\Pi^TA$ from Example \ref{exm:vsep}  and sketch
the fill-in obtained during Gaussian elimination   in Figure \ref{fig:fill}.
\end{example}
\begin{figure}
%\sidecaption
\begin{minipage}{.45\textwidth}
    $\left(
        \begin{array}{cc|cc|cc}
        3   & \nl & \nl & \nl & \nl   &  1  \\
        \nl & 3   &  \nl& \nl & \nl & \nl   \\ \hline
        \nl &  \nl& 3   & \nl &  1 & \nl \\
        \nl & \nl &  1 &  2  & \times   & \nl \\ \hline
        \nl  & 1  & \nl & \nl & 2   & \nl   \\
        \nl   & \nl & \nl & 1   & 3 & 4   
        \end{array}
    \right)$
\end{minipage}
    \caption{Fill-in with respect to $L+U$ is denoted by $\times$.}
    \label{fig:fill}
\end{figure}

% As usual we will denote directed edges $(i,j)$ by an arrow whereas if $(i,j)$ and $(j,i)$ exist we omit the arrow and just draw a line (see Figure \ref{matrixpattern}). 
%\begin{definition}
%Let $A\in\R^{n,n}$. For an edge $(i,j)$ in $G(A)$ we will write 
%$i\stackrel{G(A)}{\to} j$ or simply  $i{\to} j$ if the graph is
%clear from the context. Likewise we will write
%$i\stackrel{G(A)}{\Rightarrow} j$ or simply  $i{\Rightarrow} j$ if the
%exists a path from $i$ to $j$ in $G(A)$.
%\end{definition}

The fastest known method for predicting the filled graph $G_d(L+U)$ is Gaussian elimination. 
The situation is simplified if the
 graph is undirected. 
In the sequel we ignore the orientation of the edges and simply consider
the undirected graph $G(A)$ and $G(L+U)$, respectively.

\begin{definition}\label{def:filled-graph}
The undirected graph $G(L+U)$ that is derived from the undirected graph 
$G(A)$ by applying Theorem \ref{thr:gilbert} is called the \emph{filled graph} 
and it will be denoted by $G_f(A)$.
\end{definition}


\begin{example}{Fill-in with respect to the undirected graph}\label{exm:symfill}
When we consider the undirected graph $G(A)$ in Example \ref{exm:fill},
the pattern of $|\Pi^TA|+|\Pi^TA|^T$ and its filled graph $G_f(A)$ now equals 
$G(A)$ up to positions $(5,4)$ and $(4,5)$
(cf. Figure \ref{fig:symfill}).
\end{example}
\begin{figure}
%\sidecaption
\begin{minipage}{.45\textwidth}
    $\left(
        \begin{array}{cc|cc|cc}
         \bullet & \nl     &  \nl    &   \nl   &  \nl    & \bullet \\
         \nl     & \bullet &  \nl    &   \nl   & \bullet & \nl     \\ \hline
         \nl     & \nl     & \bullet & \bullet & \bullet & \nl     \\
         \nl     & \nl     & \bullet & \bullet & \times  & \bullet \\ \hline
         \nl     & \bullet & \bullet & \times  & \bullet & \bullet \\
         \bullet & \nl     &  \nl    & \bullet & \bullet & \bullet   
        \end{array}
    \right)$
\end{minipage}
    \caption{Entries of $G(A)$ are denoted by $\bullet$, fill-in is denoted by $\times$.}
    \label{fig:symfill}
\end{figure}

The key tool to predict the fill-in easily for the undirected graph is the
\emph{elimination tree} \cite{Liu90}. 

Recall that an undirected and connected
graph is called a \emph{tree}, if it does not contain any cycle.
Furthermore, one vertex is identified as \emph{root}.
As usual we call a vertex $j$ \emph{parent} of $i$, if there exists an edge
$(i,j)$ in the tree such that $j$ is closer to the root. In this case
$i$ is called \emph{child} of $j$. The subtree rooted at vertex $j$ is denoted
by $T(j)$ and the vertices of this subtree 
are called \emph{descendants} of $j$ whereas $j$ is called their \emph{ancestor}.
%
Initially we will define the elimination tree algorithmically 
using the depth-first-search algorithm \cite{AhoHU83}. Later we will
state a much simplified algorithm. 
\begin{definition}\label{def:etree}
Given the filled graph $G_f(A)$ the
\emph{elimination tree} $T(A)$ is defined by the following algorithm.\\
Perform a depth-first-search in $G_f(A)$ starting
from vertex $n$.\\  
When vertex $m$ is visited, choose
from its unvisited neighbors $i_1,\dots,i_k$ the index
$j$ with the largest number $j=\max\{i_1,\dots,i_k\}$ and
continue the search with $j$. \\
A leaf of the tree is reached, 
when all neighbors have already been visited.
\end{definition}
We like to point out that the application of the 
depth-first-search to $G_f(A)$ starting at vertex $n$ behaves
significantly different from other graphs.
By Theorem \ref{thr:gilbert} it follows that
as soon as we visit a vertex $m$, all its neighbors $j>m$ must have been 
visited prior to vertex $m$. Thus  the labels of the vertices are strictly 
decreasing until we reach a leaf node.
% maybe another simple examples for graphs which cannot be G_f(A)
% 1-2-3 ok, 2-1-3 not ok,  1-2 not ok
%                          | |
%                          4-3

\begin{example}{Depth-first-search}\label{exm:dfs}
We illustrate the depth-first-search using the (filled) graph in
Figure \ref{fig:matrixpattern} and the pattern from Example \ref{exm:symfill}.
The extra fill edge is marked by a bold line. 

The ongoing depth-first search visits the vertices in the order
$6\to5\to4\to 3$. Since at vertex $3$, all neighbors of $3$ are visited (and indeed have a larger number), the algorithm backtracks to $4$ and to $5$ and continues the search in the order
$5\to2$. Again all neighbors of vertex $2$ are visited (and have larger number), 
thus the algorithm backtracks to $5$ and to $6$ and continues by $6\to1$. Then the
algorithm terminates.
% Note that vertex $3$ is isolated and if the graph of $A$ is not connected one has to proceed for each connected component separately.
\end{example}
\begin{figure}[htb]
%\sidecaption
%\fbox
{
\begin{minipage}{.35\textwidth}
\includegraphics[width=\textwidth]{figures/filledgraph}
\end{minipage}
}
~~~~\hfil~~~~
%\fbox
{
\begin{minipage}{.5\textwidth}
\includegraphics[width=\textwidth]{figures/etree}
\end{minipage}
}
\caption{Filled graph (left) and elimination tree (right).}\label{fig:matrixpattern}
\end{figure}


\begin{remark}\label{rem:cross-edges}
It follows immediately from the construction of $T(A)$ and Theorem 
\ref{thr:gilbert} 
that additional edges of $G_f(A)$ which are not covered by the elimination
tree can only show up between a vertex and some of its ancestors (referred to as ``back-edges''). In contrast to that, ``cross-edges'' between unrelated vertices
do not exist. 
\end{remark}

\begin{remark}\label{rem:dependence}
One immediate consequence of Remark \ref{rem:cross-edges} is 
that triangular factors can be computed independently starting from the
leaves until the vertices meet a common parent, i.e., 
column $j$ of $L$ and $U^T$ only depend on those columns $s$
of $L$ and $U^T$ such that $s$ is a descendant
of $j$ in the elimination tree $T(A)$.
\end{remark}

\begin{example}{Elimination tree}\label{exm:etree}
We use the matrix ``west0479'' from Example \ref{exm:west0479-metis},
after maximum weight matching and multilevel nested dissection have
been applied. We use \ml's \texttt{etreeplot} to display its elimination
tree (see Figure \ref{fig:etreeplot}). The elimination tree displays the high
level of concurrency that is induced by nested dissection, since 
by Remark \ref{rem:dependence} the computations can be executed independently
at each leaf node towards the root until a common parent vertex is reached.
\end{example}
\begin{figure}
%\sidecaption
%\fbox
{
\begin{minipage}{.99\textwidth}
\includegraphics[width=\textwidth,height=0.3\textwidth]{figures/west0479-match-metis-etree}
\end{minipage}
}
\caption{Elimination tree of ``west0479'' after maximum weight matching and nested dissection are applied.}\label{fig:etreeplot}
\end{figure}





Further conclusions can be easily derived
from the elimination tree, in particular
Remark   \ref{rem:dependence} in conjunction with Theorem \ref{thr:gilbert}. 
\begin{remark}\label{rem:path-compression}
Consider some $k\in\{1,\dots,n\}$. Then there exists a (fill) edge $(j,k)$ 
with $j<k$ if and only if there exists a common
descendant $i$ of $k,j$ in $T(A)$ such that $a_{ik}\not=0$.
This follows from the fact that once $a_{ik}\not=0$, by Theorem \ref{thr:gilbert}
this induces (fill) edges $(j,k)$ in the filled graph $G_f(A)$ for all nodes
$j$ between $i$ and $k$ in the elimination tree $T(A)$, i.e., for all ancestors
of $i$ that are also descendants of $k$. This way, $i$ propagates fill-edges
along the branch from $i$ to $k$ in $T(A)$ and the information $a_{ik}\not=0$ can be
used as path compression to advance from $i$ towards $k$ along the elimination
tree.
\end{remark}
\begin{example}{Path compression}\label{exm:pc}
Consider the graph and the elimination tree from 
Figure \ref{fig:matrixpattern}. Since there exists the edge $(3,5)$ in $G(A)$,
therefore another (fill) edge $(4,5)$ must exist. Similarly, the same conclusion can be drawn
from the existence of the edge $(4,6)$ (here not a fill edge, but a regular edge).
\end{example}


The elimination tree itself can be easily described by a vector $p$ of length
$n$ such that for any $i<n$, $p_i$ denotes the parent node while $p_n=0$
corresponds to the root. 
Consider some step $k$ with $a_{ik}\not=0$, for some $i<k$. 
By Remark \ref{rem:path-compression}, $i$ must be a descendant of $k$
and there could be further ancestors $j$ of $i$ which are also descendants of $k$.
Possibly not all ancestors of $i$ have been assigned a parent node so far.
Thus we can replace $i$ by $j=p_i$ until we end up with $p_j=0$ or $p_j\geqslant k$.
This way we traverse $T(A)$ from $i$ towards to $k$ until we have found the child 
node $j$ of $k$. If the parent of $j$ has not been assigned to $j$ yet, 
then $p_j=0$ and $k$ must be the parent of $j$. If some $l<k$ were the parent of 
$j$, then we would have assigned $l$ as parent of $j$ in an earlier step $l<k$.
In this case we set $p_j\leftarrow k$. Otherwise, if $p_j\geqslant k$, then we have already
assigned $j$'s parent in an earlier step $l<k$.

\begin{example}{Computation of parent nodes}\label{exm:parent_nodes}
Consider the elimination tree $T(A)$ from Figure \ref{fig:matrixpattern}.
Unless $k=4$, no parents have been assigned, i.e. $p_i=0$ for all $i$.

Now for $k=4$ we have 
$a_{34}\not=0$ and using the fact that $p_3=0$ implies that we have to set
$a_3=p_3\leftarrow 4$.

For $k=5$, 
$a_{25}\not=0$ and again $p_2=0$ requires to set $a_2=p_2\leftarrow 5$.
Next, $a_{35}\not=0$, path compression enables $a_3\leftarrow 5$ and
after another loop we obtain $a_4=p_4\leftarrow 5$.

Finally, if $k=6$, we have $a_{16}\not=0$
and immediately obtain $a_1=p_1\leftarrow 6$.
Since $a_{46}\not=0$,  a path compression is applied  which yields
$a_4\leftarrow 6$ and in the next step we set $a_5=p_5\leftarrow 6$.
At last $a_{56}\not=0$ does not cause further changes.

In total we have $p=[6, 5, 4, 5, 6, 0]$ which perfectly reveals the parent
properties of the elimination trees in  Figure \ref{fig:matrixpattern}.
\end{example}

By Remark \ref{rem:path-compression} (cf. \cite{Tar83,Dav06}), 
we can also make use of path compression.
Since our goal is to traverse the branch of the elimination tree from $i$ to $k$
as fast as possible, any ancestor $j=a_i$ of $i$ would be sufficient. With the
same argument as before, an ancestor $a_j=0$ would refer to a vertex that does
not have a parent yet. In this case we can again set $p_j\leftarrow k$. Moreover,
$k$ is always an ancestor of $a_i$.




The algorithm including path compression can be summarized as follows 
(see also  \cite{Liu90,Dav06}).
\begin{programcode}{Computation of the elimination tree}\label{alg:etree}
\begin{algorithmic}[1]
  \Require $A\in\R^{n,n}$ such that $A$ has the same pattern as $|A|+|A|^T$.
  \Ensure vector $p\in\R^n$ such that $p_i$ is the parent of $i$, $i=1,\dots,n-1$,
except $p_n=0$.
  \State let $a\in\R^n$ be an auxiliary vector used for path compression.
  \State $p\leftarrow 0, a\leftarrow 0$
    \For{$k=2,\dots,n$}
        \For{all $i<k$ such that $a_{ik}\not=0$}
            \While{$i\not=0$ and $i<k$}
                  \State $j\leftarrow a_i$
                  \State $a_i\leftarrow k$
                  \If{$j=0$} 
                     \State $p_i\leftarrow k$
                  \EndIf   
                  \State $i\leftarrow j$
            \EndWhile
        \EndFor
    \EndFor
\end{algorithmic}
\end{programcode}




\subsection{The supernodal approach}
We have already seen that the elimination tree reveals information about
concurrency. It is further useful to determine the fill-in $L$ and $U^T$.
This information can be computed from the elimination tree $T(A)$ together
with $G(A)$. The basis for determining the fill-in in each column is 
again Remark \ref{rem:path-compression}. Suppose we are interested in the
nonzero entries of column $j$ of $L$ and $U^T$. Then for all descendants of $j$,
i.e. the nodes of the subtree $T(j)$ rooted at vertex $j$, a nonzero entry
$a_{ik}\not=0$ also implies $l_{kj}\not=0$. Thus, starting at any leaf $i$,
we obtain its fill by all $a_{ik}\not=0$ such that $k>i$ and when we move forward
from $i$ to its parent $j$, vertex $j$ will inherit the fill from node $i$ for
all $k>j$ plus the nonzero entries given by $a_{jk}\not=0$ such that $k>j$.
When we reach a common parent node $k$ with multiple children, the same argument
applies using the union of fill-in greater than $k$ from its children together
with the nonzero entries $a_{kl}\not=0$ such that $l>k$.
We summarize this result in a very simple algorithm
\begin{programcode}{Computation of fill-in}\label{alg:compute_pattern}
\begin{algorithmic}[1]
  \Require $A\in\R^{n,n}$ such that $A$ has the same pattern as $|A|+|A|^T$.
  \Ensure sparse strict lower triangular pattern $P\in\R^{n,n}$ with
  same pattern as $L$, $U^T$.
  \State compute parent array $p$ of the elimination tree $T(A)$
  \For{$j=1,\dots,n$}
      \State supplement nonzeros of column $j$ of $P$ with all $i>j$ such that $a_{ij}\not=0$ 
      \State $k=p_j$
      \If{$k>0$} 
         \State supplement nonzeros of column $k$ of $P$ with nonzeros of column $j$ of $P$ greater than $k$
      \EndIf   
  \EndFor
\end{algorithmic}
\end{programcode}



Algorithm \ref{alg:compute_pattern} only deals with the fill pattern. 
One additional aspect that allows to raise efficiency
and to speed up the numerical factorization significantly 
is to detect dense submatrices in the factorization.
Block structures  allow to collect parts
of the matrix in dense blocks and to treat them commonly using 
dense matrix kernels such as level-3 BLAS and LAPACK \cite{DodL85,DonDHH88}.

Dense blocks can be read off from the elimination tree employing
Algorithm \ref{alg:compute_pattern}.
\begin{definition}\label{def:supernode}
Denote by $\mathcal{P}_j$ the nonzero indices of column $j$ of $P$
as computed by Algorithm \ref{alg:compute_pattern}.
A sequence $k,k+1,\dots,k+s-1$ is called \emph{supernode} of size $s$
if the columns of $\mathcal{P}_{j}=\mathcal{P}_{j+1}\cup \{j+1\}$
for all $j=k,\dots,k+s-2$.
\end{definition}
In simple words, Definition \ref{def:supernode} states that for a supernode
$s$ subsequent columns can be grouped together in one dense block with a triangular
diagonal block and a dense subdiagonal block since they perfectly match the 
associated trapezoidal shape. We can thus easily supplement 
Algorithm \ref{alg:compute_pattern} with a supernode detection.
\begin{programcode}{Computation of fill-in and supernodes}\label{alg:compute_supernode}
\begin{algorithmic}[1]
  \Require $A\in\R^{n,n}$ such that $A$ has the same pattern as $|A|+|A|^T$.
  \Ensure sparse strict lower triangular pattern $P\in\R^{n,n}$ with
  same pattern as $L$, $U^T$ as well as column size $s\in\R^m$ of each supernode.
  \State compute parent array $p$ of the elimination tree $T(A)$
  \State $m\leftarrow0$ 
  \For{$j=1,\dots,n$}
      \State supplement nonzeros of column $j$ of $P$ with all $i>j$ such that $a_{ij}\not=0$ 
      \State denote by $r$ the number of entries in column $j$ of $P$
      \If{$j>1$ and $j=p_{j-1}$ and $s_m+r=l$}
         \State $s_m\leftarrow s_m+1$ \Comment{continue current supernode}
      \Else
         \State $m\leftarrow m+1$, $s_m\leftarrow 1$, $l\leftarrow r$ \Comment{start new supernode}
      \EndIf
      \State $k=p_j$
      \If{$k>0$} 
         \State supplement nonzeros of column $k$ of $P$ with nonzeros of column $j$ of $P$ greater than $k$
      \EndIf   
  \EndFor
\end{algorithmic}
\end{programcode}





\begin{example}{Supernode Computation}\label{exm:supernode_computation}
To illustrate the use of supernodes, we consider the matrix pattern
from Figure \ref{fig:symfill} and illustrate the underlying
dense block structure in Figure \ref{fig:supernode}.
Supernodes are the columns $1$, $2$, $3$ as scalar columns as well as columns
$4$-$6$ as one single supernode.
\end{example}
\begin{figure}
%\sidecaption
\begin{minipage}{.45\textwidth}
    $\left(
        \begin{array}{cccccc}
         \\[-1.5ex]\cline{1-1}
         \multicolumn{1}{|c|}{\bullet}&       &       &       &       &         \\ \cline{1-2}
         \nl    &\multicolumn{1}{|c|}{\bullet}&       &       &       &         \\ \cline{2-3}
         \nl    &\nl    &\multicolumn{1}{|c|}{\bullet}&       &       &         \\ \cline{4-4}
         \nl    &\nl    & \multicolumn{1}{|c|}{\bullet}   &\multicolumn{1}{|c|}{\bullet}&       &         \\ \cline{2-2}\cline{5-5}
         \nl    &\multicolumn{1}{|c|}{\bullet}& \multicolumn{1}{|c|}{\bullet}   &\multicolumn{1}{|c}{\times}&\multicolumn{1}{c|}{\bullet}&         \\ \cline{1-3}\cline{6-6}
         \multicolumn{1}{|c|}{\bullet}&\nl& \nl   &\multicolumn{1}{|c}{\bullet}&\bullet& \multicolumn{1}{c|}{\bullet}\\
\cline{1-1}\cline{4-6}
 \end{array}
    \right)$
\end{minipage}
    \caption{Supernodes in the triangular factor.}
    \label{fig:supernode}
\end{figure}

Supernodes form the basis of several improvements, e.g.,
a supernode can be stored as one or two dense matrices. 
Beside the storage scheme as dense matrices, the nonzero row indices
for these blocks need only be stored once.
Next the use of dense submatrices allows the usage of dense matrix kernels
using level-3 BLAS
\cite{DodL85,DonDHH88}. 
\begin{example}{Supernodes}\label{exm:supernodes}
We use the matrix ``west0479'' from Example \ref{exm:west0479-metis},
after maximum weight matching and multilevel nested dissection have
been applied. 
We use its undirected graph to compute the supernodal
structure. Certainly, since the matrix is nonsymmetric, the block structure
is only sub-optimal. We display the supernodal structure for the associated
Cholesky factor, i.e., for the Cholesky factor of a symmetric positive definite
matrix with same undirected graph as our matrix (see
left part of Figure \ref{fig:supernodal_structure}). Furthermore, we display
the supernodal structure for the factors $L$ and $U$ computed from the
nonsymmetric matrix without pivoting (see right part of Figure \ref{fig:supernodal_structure}).
\end{example}
\begin{figure}
% \sidecaption
\begin{minipage}{.48\textwidth}
 \begin{center}
\includegraphics[width=0.99\textwidth]{figures/west0479-match-metis-chol-super} 
 \end{center}
\end{minipage}
~
\begin{minipage}{.48\textwidth}
  \begin{center}
\includegraphics[width=0.99\textwidth]{figures/west0479-match-metis-lu-super} 
 \end{center}  
\end{minipage}
    \caption{Supernodal structure. Left: vertical lines display the blocking of the
supernodes with respect to the associated Cholesky factor. Right: 
vertical and horizontal lines display the blocking of the
supernodes applied to $L$ and $U$.}
    \label{fig:supernodal_structure}
\end{figure}

While the construction of supernodes is fairly easy in the symmetric case,
its generalization for the nonsymmetric case is significantly harder, since one
has to deal with pivoting in each step of Gaussian elimination.
In this case one uses the column elimination tree \cite{GeoN85}.



% ------------------repetition, therefore removed !!! ----------------------------
%\begin{programcode}{Computation of fill-in and supernodes}\label{alg:compute_supernode}
%\begin{algorithmic}[1]
%  \Require $A\in\R^{n,n}$ such that $A$ has the same pattern as $|A|+|A|^T$.
%  \Ensure sparse strict lower triangular pattern $P\in\R^{n,n}$ with
%  same pattern as $L$, $U^T$ as well as column size $s\in\R^m$ of each supernode.
%  \State compute parent array $p$ of the elimination tree $T(A)$
%  \State $m\leftarrow0$ 
%  \For{$j=1,\dots,n$}
%      \State supplement nonzeros of column $j$ of $P$ with all $i>j$ such that $a_{ij}\not=0$ 
%      \State denote by $r$ the number of entries in column $j$ of $P$
%      \If{$j>1$ and $j=p_{j-1}$ and $s_m+r=l$}
%         \State $s_m\leftarrow s_m+1$ \Comment{continue current supernode}
%      \Else
%         \State $m\leftarrow m+1$, $s_m\leftarrow 1$, $l\leftarrow r$ \Comment{start new supernode}
%      \EndIf
%      \State $k=p_j$
%      \If{$k>0$} 
%         \State supplement nonzeros of column $k$ of $P$ with nonzeros of column $j$ of $P$ greater than $p$
%      \EndIf   
%  \EndFor
%\end{algorithmic}
%\end{programcode}

\section{Sparse Direct Solvers --- Supernodal Data Structures}
%%%\label{sec:BLAS3}
\label{sec:parallel}

High-performance sparse solver libraries have been a very important part of
scientific and engineering computing for years, and their importance
continues to grow as microprocessor architectures become more complex
and software libraries become better designed to integrate easily
within applications. Despite the fact that there are various science
and engineering applications, the underlying algorithms typically have
remarkable similarities, especially those algorithms that are most
challenging to implement well in parallel. It is not too strong a
statement to say that these software libraries are essential to the
broad success of scalable high-performance computing in computational
sciences.  In this section we demonstrate the benefit of supernodal data structures within the 
sparse solver package PARDISO~\cite{schenk-2004}. We illustrate it by using 
the triangular solution process. The forward and backward substitution is performed
column wise with respect to the columns of $L$, starting with the
first column, as depicted in Figure~\ref{algo:triangular}.
The data dependencies here allow to store vectors $y$, $z$, $b$, and $x$ in only one
vector $r$. When column $j$ is reached, $r_j$ contains the solution for $y_j$. 
All other elements of $L$ in this column, i.\,e.\ $L_{ij}$ with $i = j + 1,
\ldots, N$, are used to update the remaining entries in $r$ by 
%
\be
  r_i = r_i - r_j L_{ij}.
  \label{eq:algo:fw:pardiso}
\ee
%
The backward substitution with~$L^T$ will take place row wise, since we
use $L$ and perform the substitution column wise with respect to $L$, as shown in the lower part of
Figure~\ref{algo:triangular}.  In contrast to the forward substitution the
iteration over columns starts at the last column $N$ and proceeds to
the first one.  If column $j$ is reached, then $r_j$, which contains the $j$-component of the solution vector $x_j$,
is computed by subtracting the dot-product of the remaining elements in
the column $L_{ij}$ and the corresponding elements of $r_i$ with $i =
j + 1, \ldots, N$ from it:
%
\be
  r_j = r_j - r_i L_{ij} .
  \label{eq:algo:bw:pardiso}
\ee
%
After all columns have been processed $r$ contains the required solution $x$. It is important to note that
line 5 represents in both substitutions an indexed DAXPY and indexed
DDOT kernel operations that has to be computed during the streaming 
operations of the vector $r$ and the column $j$ of the numerical factor $L$. 
As we are dealing with sparse matrices it makes no sense to store the lower
triangular matrix $L$ as a dense matrix.
Hence PARDISO uses its own data structure to store $L$, as shown in
Figure~\ref{fig:algo:ds}. 
%
\begin{figure*}[t]
    \centering
    \begin{minipage}{.35\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth,clip=true]{images/forward-small}
    \end{minipage}%
    \begin{minipage}{0.65\textwidth}
        \centering
  \begin{algorithmic}[1]
    \Procedure{Sparse forward substitution}{}
            \For{j = 0; j < n; j++}\label{algo:fw:cholmod}
                \For{i = \nxlnz[j]; i < \nxlnz[j+1]; i++}
                   \State row = \nindx[i]
                   \State \nr[row] -=  \nr[j] * \nlnz[i] \Comment{indexed DAXPY}
            \EndFor\label{algo:fw:cholmod:rloop:end}
      \EndFor
    \EndProcedure
  \end{algorithmic}
    \end{minipage}

\bigskip

    \centering
    \begin{minipage}{.35\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth,clip=true]{images/backward-small}
    \end{minipage}%
    \begin{minipage}{0.65\textwidth}
        \centering
  \begin{algorithmic}[1]
    \Procedure{Sparse backward substitution}{}
            \For{j =  n;  j > 0; j - -}\label{algo:bw:cholmod}
                \For{i = \nxlnz[j]; i < \nxlnz[j+1]; i++}
                   \State row = \nindx[i]
                   \State \nr[j] -=  \nr[row] * \nlnz[i] \Comment{indexed DDOT}
            \EndFor\label{algo:bw:cholmod:rloop:end}
      \EndFor
    \EndProcedure
  \end{algorithmic}
    \end{minipage}
  \caption{Sparse triangular substitution in CSC format based on indexed DAXPY/DDOT kernel operations.}
  \label{algo:triangular}
\end{figure*}


\begin{figure}[t]
  \centering
    \includegraphics[width=0.5\textwidth,clip=true]{images/parts-panels-separator}
  \caption{Sparse matrix data structures in PARDISO. Adjacent columns of $L$ exhibiting the same
structure form panels also known as supernodes. 
Groups of panels which touch independent elements of the right hand side $r$ are
parts. The last part in the lower triangular matrix $L$ is called separator.}
  \label{fig:algo:ds}
\end{figure}


\begin{algorithm}[t]
  \begin{algorithmic}[1]
    \Procedure{Forward}{}
      \For{part $o$ in parts} \Comment{parallel execution}
        \For{panel p in part $p$}
          \For{\textcolor{blue}{column $j$ in panel}} \Comment{unroll} \label{alg:fw:1}
            \State i = \nxindx{}[p] + offset
          
            \For{k = \nxlnz[j] + offset; k < sep; ++k}\label{algo:fw:rloop}
                \State row = \nindx[i++]
                \State \nr[row] - =  \nr[j] \nlnz[k] \Comment{indexed DAXPY} 
            \EndFor\label{algo:fw:rloop:end}
            \For{k = sep + 1; k < \nxlnz[j+1]; ++k}\label{algo:fw:seploop}
                \State row = \nindx[i++]
                \State \ntemp[row,p] -=  \nr[j] \nlnz[k] \Comment{indexed DAXPY}
            \EndFor\label{algo:fw:seploop:end}
          \EndFor
        \EndFor
      \EndFor
      \State r[i] = r[i] - sum(\ntemp[i,:])  \Comment{gather temporary arrays}
      \For{panel p in separator} \Comment{serial execution}
        \For{\textcolor{blue}{column $j$ in panel}} \Comment{unroll}\label{alg:fw:2}
            \State i = \nxindx[p] + offset
          
            \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; ++k}
                \State row = \nindx[i++]
                \State \nr[row] -=  \nr[j] \nlnz[k] \Comment{indexed DAXPY}
            \EndFor
        \EndFor
      \EndFor
    \EndProcedure
  \end{algorithmic}
  \caption{Forward substitution in PARDISO. Note that in case of serial
execution separated updates to temporary arrays in line
\ref{algo:fw:seploop}-\ref{algo:fw:seploop:end} are not necessary
and can be handled via the loop in lines
\ref{algo:fw:rloop}-\ref{algo:fw:rloop:end}.}
  \label{alg:algo:fw}
\end{algorithm}

Adjacent columns exhibiting the same row sparsity structure form a \textit{panel}, also known
as \textit{supernode}.
A panel's column count is called the \textit{panel size} $n_p$.
The columns of a panel are stored consecutively in memory excluding the zero
entries. 
Note that columns of panels are padded in the front with zeros so they get the 
same length as the first column inside their panel. The padding is of utmost performance
for the PARDISO solver to use Level-3 BLAS and LAPACK functionalities~\cite{20.500.11850/144477}.
 Furthermore panels are stored consecutively in the \vlnz{} array. 
Row and column information is now stored in accompanying arrays.
The \texttt{xsuper} array stores for each panel the index of its first column. 
Also note that here column indices are the running count of nonzero columns.
Column indices are used as indices into \vxlnz{} array to lookup the start of
the column in the \vlnz{} array which contains the numerical values of the factor $L$.
To determine the row index of a column's element an additional array \vindx{} is
used, which holds for each panel the row indices.
The start of a panel inside \vindx{} is found via \vxindx{} array.
The first row index of panel~$p$ is \vindx\texttt{[\vxindx[p]]}.
For serial execution this information is enough. 
However, during parallel forward/backward substitution concurrent updates to
the same entry of \vr{} must be avoided.
The \textit{parts} structure contains the start (and end) indices of the panels which can
be updated independently as they do not touch the same entries of $r$.
Two parts, colored blue and orange, are shown in Figure~\ref{fig:algo:ds}.
The last part in the bottom right corner of $L$ is special and is called the 
\textit{separator} and is colored green.
%
Parts which would touch entries of \vr{} in the range of the separator perform 
their updates into separate temporary arrays \vtemp{}.
Before the separator is then serially updated, the results of the temporary
arrays are gathered back into \vr{}. 
The backward substitution works the same, just reversed and
only updates to different temporary arrays are not required.
The complete forward substitution and backward substitution  is listed in Algorithms~\ref{alg:algo:fw} and \ref{alg:algo:bw}.
%


 \begin{algorithm}[tp]
   \begin{algorithmic}[1]
     \Procedure{Backward}{}
       \For{panel $p$ in sep. rev.} \Comment{serial execution}
         \For{\textcolor{blue}{col. $j$ in panel $p$ rev.}} \Comment{unroll}\label{alg:bw:1}
            \State i = \nxindx[p] + offset
            \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; ++k}
                \State row = \nindx[i++]
                \State \nr[j] -= \nr[row] \nlnz[k] \Comment{indexed DDOT}
            \EndFor

            \State offset = offset - 1
          \EndFor
        \EndFor
        \For{part in parts} \Comment{parallel execution}
          \For{panel $p$ in part rev.}
            \For{\textcolor{blue}{col. $j$ in panel $p$ rev.}} \Comment{unroll}\label{alg:bw:2}

              \State i = \nxindx[p] + offset

              \For{k = \nxlnz[j] + offset; k < \nxlnz[j+1]; ++k}
                \State row = \nindx[i++]
                \State \nr[j] -=  \nr[row] \nlnz[k] \Comment{indexed DDOT}
              \EndFor

              \State offset = offset - 1

            \EndFor
          \EndFor
        \EndFor
        \EndProcedure
   \end{algorithmic}
   \caption{Backward substitution in PARDISO. Separator (sep.), parts, and
panels are iterated over in reversed (rev.) order.}
   \label{alg:algo:bw}
\end{algorithm}


\section{Application --- Circuit Simulation}
~\label{sec:appl} 

In this section we demonstrate how these developments in sparse direct linear solvers
have advanced integrated circuit simulations.  Integrated circuits are composed 
of interconnected transistors. The interconnects are modeled primarily with 
resistors, capacitors, and inductors. The interconnects route signals through the circuit, 
and also deliver power. Circuit equations arise out of Kirchhoff's current
law, applied at each node, and are generally nonlinear
differential-algebraic equations.  In transient simulation of the
circuit, the differential portion is handled by discretizing the time
derivative of the node charge by an implicit integration formula.  The
associated set of nonlinear equations is handled through use of
quasi-Newton methods or continuation methods, which change the
nonlinear problem into a series of linear algebraic solutions.  Each
component in the circuit contributes only to a few equations.  Hence
the resulting systems of linear algebraic equations are extremely
sparse, and most reliably solved by using direct sparse matrix
techniques.  Circuit simulation matrices are peculiar in the universe
of matrices, having the following characteristics~\cite{davis:klu}:

\begin{itemize}
\item they are nonsymmetric, although often nearly structurally
  symmetric;
\item they have a few dense rows and columns (e.g., power and ground
  connections);
\item they are {\em very} sparse and the straightforward usage of
                 BLAS routines (as in SuperLU\cite{superlu}) may 
                 be ineffective;
\item their LU factors remain sparse if well-ordered;
\item they can have high fill-in if ordered with typical strategies;
\item and being unstructured, the highly irregular memory access causes
  factorization to proceed only at a few percent of the peak flop-rate.
\end{itemize}

Circuit simulation matrices also vary from being positive definite to
being {\em extremely} ill-conditioned, making pivoting for stability
important also.  As circuit size increases, and depending on how much
of the interconnect is modeled, sparse matrix factorization is the
dominant cost in the transient analysis.

To overcome the complexity of matrix factorization a new class of
simulators arose in the 1990s, called fast-SPICE \cite{Rewienski2011APO}.
These simulators partition the circuit into subcircuits and use a 
variety of techniques, including model order reduction and multirate
integration, to overcome the matrix
bottleneck.  However, the resulting simulation methods generally incur
unacceptable errors for analog and tightly coupled circuits. As
accuracy demands increase, these techniques become much slower than
traditional SPICE methods. Even so, since much of the research effort
was directed at fast-SPICE simulators, it brought some relief from
impossibly slow simulations when some accuracy trade-off was
acceptable.  Because these simulators partitioned the circuit, and did
not require the simultaneous solution of the entire system of linear
equations at any given time, they did not push the state-of-the-art in
sparse matrix solvers.

Starting in the mid-2000s, increasing demands
on accuracy, due to advancing semiconductor technology, brought
attention back to traditional SPICE techniques.  This was aided by the
proliferation of multicore CPUs. Parallel circuit simulation, an area
of much research focus in the 1980s and 1990s, but not particularly in
practice, received renewed interest as a way to speed up simulation
without sacrificing accuracy.  Along with improved implementations to
avoid cache misses, rearchitecture of code for parallel computing,
and better techniques for exploitation of circuit latency, improved
sparse matrix solvers, most notably the release of KLU
\cite{davis:klu}, played a crucial role in expanding the utility of
SPICE. 

Along with the ability to simulate ever larger circuits with full
SPICE accuracy came the opportunity to further improve sparse matrix
techniques.  A sparse matrix package for transient simulation
needs to have the following features:

\begin{itemize}
\item must be parallel;
\item fast matrix reordering
\item incremental update of the $L$ and $U$ factors when only a few
  nonzeros change;
\item fast computation of the diagonal entries of the inverse matrix;
\item fast computation of Schur-complements for a submatrix;
\item allow for multiple $LU$ factors of the same structure to be stored;
\item use the best-in-class method across the spectrum of sparsity;
\item use iterative solvers with fast construction of sparse preconditioners;
\item run on various hardware platforms (e.g. GPU acceleration).
\end{itemize}

Some of these features must be available in a single package.  Others,
such as iterative solvers and construction of preconditioners, can be
implemented with a combination of different packages. 
The PARDISO solver\footnote{The PARDISO solver is available from 
\url{http://www.pardiso-project.org}.} 
stands out as a package that does most of these very well.
Here we touch on a few of these features.  

When applied in the simulation of very large circuits, the difference between a 
``good'' and a ``bad'' matrix ordering can be the difference between seconds and days.
PARDISO offers AMD and nested-dissection methods for matrix ordering, as well as 
permitting user-defined ordering. Because the matrix re-ordering method which has been 
used most often in circuit simulation is due to Markowitz \cite{markowitz}, and because
modern sparse matrix packages do not include this ordering method, we
briefly describe it here.  The Markowitz method is quite well-adapted for circuit
simulation.  Some desirable aspects of the typical implementation of the Markowitz method,
as opposed to the MD variants, are that it works for
nonsymmetric matrices and combines pivot choice with numerical
decomposition, such that a pivot choice is a numerically ``good''
pivot which generates in a local sense the least fill-in at that step
of the decomposition.  Choosing pivots based on the Markowitz score 
often produces very good results: near-minimal fill-in, unfortunately at the cost of an
$O(n^3)$ algorithm (for dense blocks).  
Even though the Markowitz algorithm has some good properties when applied
to circuit matrices, the complexity of the algorithm has become quite
burdensome.  When SPICE~\cite{nagel:spice2} was originally conceived,
a hundred-node circuit was huge and the Markowitz algorithm was not a
problem.  Now we routinely see netlists with hundreds of thousands of
nodes and postlayout netlists with millions of elements.  As matrix
order and element counts increase, Markowitz reordering time can
become an obstruction. Even as improved implementations of the Markowitz
method have extended its reach, AMD and nested-dissection 
have become the mainstay of simulation of large denser-than-usual matrices.

Next we turn our attention to parallel performance. 
While KLU remains a benchmark for serial
solvers, for parallel solvers, MKL-PARDISO is often cited as the
benchmark~\cite{Booth2017, Chen2013}.  To give the reader a sense of
the progress in parallel sparse matrix methods, in Figure
\ref{fig:mklvs62} we compare KLU, PARDISO (Version 6.2) to MKL-PARDISO on up to 16 
cores  on an Intel Xeon E7-4880 architecture with 2.5 GHz processors.

\begin{figure}[t]
\newif\ifrjYLabel
\rjYLabelfalse
\newif\ifrjXTicks
\rjXTicksfalse
\newif\ifrjLegend
\rjLegendfalse
\noindent
\\
\def \rjDataFileName {figures/RJGraphs/circuit5M_DC.dat}
\def \rjTitle {circuit5M\_DC}
\rjYLabeltrue
\input{figures/RJGraphs/barPlot_all3.tex}
\def \rjDataFileName {figures/RJGraphs/circuit5M.dat}
\def \rjTitle {circuit5M}
\rjYLabelfalse
\input{figures/RJGraphs/barPlot_all3.tex}
\def \rjDataFileName {figures/RJGraphs/Freescale.dat}
\def \rjTitle {Freescale}
\rjLegendtrue
\input{figures/RJGraphs/barPlot_all3.tex}
\\
\def \rjDataFileName {figures/RJGraphs/Freescale2.dat}
\def \rjTitle {Freescale2}
\rjYLabeltrue
\rjXTickstrue
\rjLegendfalse
\input{figures/RJGraphs/barPlot_all3.tex}
\def \rjDataFileName {figures/RJGraphs/FullChip.dat}
\def \rjTitle {FullChip}
\rjYLabelfalse
\input{figures/RJGraphs/barPlot_all3.tex}
\def \rjDataFileName {figures/RJGraphs/memchip.dat}
\def \rjTitle {memchip}
\input{figures/RJGraphs/barPlot_all3.tex}
\caption{Performance improvements of PARDISO 6.2 against Intel MKL PARDISO for various circuit simulation matrices.}
     \label{fig:mklvs62}
\end{figure}

Some of the matrices here can be obtained from the SuiteSparse Matrix
Collection, and arise in transistor level full-chip and memory array
simulations. It is clear that implementation of sparse matrix solvers
has improved significantly over the years.  

Exploiting latency in all parts of the SPICE algorithm is very important 
in enabling accurate circuit simulation, especially as the circuit size 
increases.  By latency, we mean that only a few entries in the matrix change from one
\begin{wrapfigure}{r}{0.5\textwidth}
     \centering
     %\input{data.tex}
     \includegraphics[width=0.5\textwidth]{figures/update} 

     \caption{Regression analysis on the rank-$k$ update $LU$
       factorization in PARDISO. }
     \label{fig:incrementalLU}
\end{wrapfigure}
Newton iteration to the next, and from one timepoint to the next. As
the matrix depends on the time-step, some simulators hold the
time-steps constant as much as feasible to allow increased reuse of
matrix factorizations. The nonzero entries of a matrix change only
when the transistors and other nonlinear devices change their
operation point. In most circuits, very few devices change state from
one iteration to the next and from one time-step to the
next. Nonzeros contributed by entirely linear components do not change
value during the simulation. This makes incremental LU
factorization a very useful feature of any matrix solver used in
circuit simulation.  As of April 2019 the version PARDISO 6.2
 has a very efficient exploitation of
incremental LU factorization, both serial and parallel.   In
Figure~\ref{fig:incrementalLU} we show that PARDISO scales linearly
with number of updated columns, and also scales well with number of
cores. Here, the series of matrices were obtained from a full
simulation of a post-layout circuit that includes all interconnects, 
power- and ground-networks). The factorization time is plotted against the number of
columns that changed compared to the previous factorization.
The scatter plot shows the number of
rank-$k$ update and the corresponding factorization time in
       milliseconds. The regression analysis clearly demonstrates a
       linear trend both for the single and the multiple core
       versions. The dashed line shows the time for the full
       factorization.

Another recent useful feature in PARDISO is parallel selective inverse matrix computation as
demonstrated in Table~\ref{table:bench_matrices}.
In circuit simulation, the diagonal of the inverse matrix is the
driving point impedance. It is often required to flag nodes in the
circuit with very high driving point impedance. Such nodes would
indicate failed interfaces between different subcircuits, leading to
undefined state and high current leakage and power dissipation. A
naive approach to this is to solve for the driving point impedance,
the diagonal of the inverse matrix, by $N$ triangular solves. This is
sometimes unacceptably expensive even with exploiting the sparsity of the
right hand side, and minimizing the number of entries needed in the diagonal of the inverse.
To bypass this complexity, heuristics to compute the impedance of 
connected components are used. But this is error prone with many
false positives and also false negatives. In the circuit Freescale, PARDISO, e.g., finished the
required impedance calculations in 11.9 seconds compared to the
traditional computation that consumed 162.9 hours.


\begin{table}[t]
	\centering
%	\footnotesize
	\caption{Details of the benchmark matrices. 'N' is the number of matrix rows and 'nnz' is the number of nonzeros. The table
                shows the fill-in factor related to the numbers of nonzeros in $\frac{L+U}{A}$, the time for computing all diagonal elements 
                of the inverse $A^{-1}$ using $N$ multiple forward/backward substitution in hours, and  using the selected inverse method in 
		PARDISO for computing all diagonal elements of  the inverse $A^{-1}$ in seconds.}
	\begin{center}
\begin{tabular}{|l|r|r|c|r|r|}\hline
\multicolumn{1}{|c|}{Matrix}  & 
\multicolumn{1}{c|}{N}       &
\multicolumn{1}{c|}{nnz$(A)$}&
\multicolumn{1}{c|}{nnz$(\frac{L+U}{A})$} &  
\multicolumn{1}{c|}{$A^{-1}$} & 
\multicolumn{1}{c|}{Selected $A^{-1}$}  \\\hline
{circuit5M\_DC}	&  3,523,317   & 19,194,193	&  2.87  &  82.3 h.    &  1.3 s.\\
{circuit5M}	&  5,558,326   & 59,524,291	&  1.04  & 371.1 h.    &  2.1 s. \\
{Freescale}	&  3,428,755   & 18,920,347	&  2.94  & 89.8 h.     &  1.0 s. \\
{Freescale2}	&  2,999,349   & 23,042,677	&  2.92  & 8.5  h.      &  1.2 s. \\
{FullChip}	&  2,987,012   & 26,621,990	&  7.41  & 162.9 h.      &  11.9 s. \\
{memchip}	&  2,707,524   & 14,810,202	&  4.40  & 62.5 h.     & 0.9 s. \\\hline
%#TABLE_DATA#

\end{tabular}
 \label{table:bench_matrices}
	\end{center}
\end{table}



The productivity gap in simulation continues to grow, and challenges
remain.  Signoff simulations demand 10X speedup in sparse matrix
factorization. Simply using more cores does not help unless the matrices
are very large and complex. For a majority of simulations, scaling beyond
8 cores is difficult. As a result, some of these
simulations can take a few months to complete, making them essentially
impossible. Some of the problems in parallelizing sparse matrix
operations for circuit simulation are fundamental. Others may be related
to implementation.  Research on sparse matrix factorization for circuit simulation
continues to draw attention, especially in the area of acceleration
with Intel's many integrated core (MIC) architecture \cite{Booth2017}
and GPUs \cite{Chen2015, Nakhla2018}. Other techniques for
acceleration include improved preconditioners for iterative solvers
\cite{Feng2015}. We are presently addressing the need for runtime selection of
optimal strategies for factorization, and also GPU acceleration. Given
that circuits present a wide spectrum of matrices, no matter how we
categorize them, it is possible to obtain a solver that is 2-10$\times$
better on a given problem.  Improvements in parallel sparse matrix
factorization targeted at circuit simulation is more necessary today
than ever and will continue to drive applicability of traditional
SPICE simulation methods.  Availability of sparse matrix packages such
as PARDISO that completely satisfy the needs of various circuit
simulation methods is necessary for continued performance gains.

\end{comment}
%todo end comment

%\nocite{*}

%\appendix %optional, use only if you have an appendix
%
%\chapter{\todol{todo}}
%\section{\todol{It's over\dots}}

\backmatter
\bibliographystyle{plainnat}
%\bibliography{biblio}
%\bibliography{../solve,../merged_all}
\bibliography{biblio,solve,merged_all}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
