\chapter{Application of Modified Roofline Model on Sparse Direct Solve}
\label{sec:mrm}

% \begin{table}[tp]
%   \small
%   \centering
%   \begin{tabular}{llcRRRccRRR}
%   \hline
%   \multirow{2}{*}{u}&& \multicolumn{4}{c}{forward}  && \multicolumn{4}{c}{backward} \\
%   \cline{3-6} \cline{8-11}
%    && \mcco{L1} & \mcco{L2} & \mcco{L3} & \mcco{mem} && \mcco{L1} & \mcco{L2} & \mcco{L3} & \mcco{mem}\\
%   \hline
%   1 && 14   & 4 - 14  &  4 - 14  & 4 - 6    && 10   & 4 -  10 & 4 -  10 & 4  - 6 \\ 
%   2 &&  9   & 4 -  9  &  4 -  9  & 4 - 5    &&  7   & 4 -   7 & 4 -   7 & 4  - 5 \\
%   8 && 5.25 & 4 - 5.25&  4 - 5.25& 4 - 4.25 && 4.75 & 4 -  4.75&4 - 4.75& 4  - 4.25 \\
%   \hline
%   \end{tabular}
%   \caption{Code balance $B_c$ [B/F] of forward/backward substitution for
% different unrollings (u) when
% data is fetched from the corresponding level inside the memory hierarchy. Values
% depend on actual panel size $s$ and possible cache reuse.}
%   \label{tab:mrm:bc}
% \end{table}
\begin{table}[!b]
  \centering
  \begin{tabular}{ll|rrrc|rrr}
  \hline
  substitution && \multicolumn{3}{c}{forward}  && \multicolumn{3}{c}{backward} \\
  \cline{3-5} \cline{7-9}
  \hline
  $u$    &     &  1       & 2     & 8        & & 1       & 2     & 8  \\
  $B_c$  &[B/F]& 4 - 6    & 4 - 5 & 4 - 4.25 & & 4  - 6  & 4 - 5 & 4 - 4.25 \\
  \hline
  \end{tabular}
  \caption{Code balance $B_c$ of forward/backward substitution for
unrolling factors $u$ when
the factor $L$ must be fetched from memory.
% the factor $L$ exceeds the LLC and must be fetched from memory.
Values depend on actual panel size $s$ and possible cache reuse.}
  \label{tab:mrm:bc}
\end{table}

%xx \begin{table*}[!t]
%xx   \caption{Details of Evaluated Hardware Systems.
%xx % ISA Lists the Latest Extension Supported by the Processor. 
%xx % Read Memory Bandwidth, Floating Point Instructions per Cycle (ADD+MULL and
%xx % FMA Instructions), and Machine Balance is Reported for Scalar Execution.
%xx KNL's Bandwidth Numbers are for DDR Memory.}
%xx % ECM: 2 cy L1/L2 bei HSW/BDW entgegen der Doku 
%xx % STREAM:
%xx % read = summation
%xx % KNL: no-nt, prefetch not explicitly disabled
%xx % ZEN: no-nt, AVX2, only even cores
%xx % HSW2: no-nt, avx2
%xx % HSW: no-nt, avx2
%xx % IVB: no-nt, 
%xx   \label{tab:hw}
%xx   \footnotesize
%xx  \centering
%xx %\resizebox{\textwidth}{!}{%
%xx  \begin{tabular}{p{1.9cm}llrrrrrrrrr}
%xx     \hline
%xx     name      & &  & IVB         & HSW-D      & HSW-S           & BDW           & SKX         & KNL           & ZEN-D        &  ZEN-S         \\
%xx     \hline                                                      
%xx     \multirow{3}{\linewidth}{processor name} & &  & Intel& Intel & Intel & Intel & Intel & Intel & AMD &  AMD \\
%xx       & &  & Xeon  & Xeon & Xeon      & Xeon    & Xeon  & Xeon    &
%xx ~~Ryzen 7   &  ~~EPYC      \\
%xx               & &  &\scriptsize  E5-2660 v2 &\scriptsize  E3-1240 v3
%xx &\scriptsize  E5-2695 v3      &\scriptsize  E5-2630 v4    &\scriptsize  Gold
%xx 6148   &\scriptsize  Phi 7210      &\scriptsize  1700X      &\scriptsize
%xx 745           \\
%xx %     processor & &  & Intel Xeon  & Intel Xeon & Intel Xeon      & Intel Xeon    & Intel Xeon  & Intel Xeon    & AMD Ryzen    &  AMD EPYC      \\
%xx %     name      & &  &  E5-2660 v2 & E3-1240 v3 & E5-2695 v3      & E5-2630 v4    & Gold 6148   & Phi 7210      & 7 1700X      &  745           \\
%xx     \hline                                                      
%xx     micro     & &  & Ivy Bridge  & Haswell    & Haswell         & Broadwell
%xx & Skylake     & ~Knigths       & Zen          &  Zen           \\
%xx     arch.     & &  &             &            &                 &               &             & Landing       & \\
%xx     \hline                                                    
%xx     freq    & [GHz] & & 2.2      & 3.4        & 2.3             & 2.2           & 2.4         & $\approx$ 1.3 & 3.4          &  2.3           \\
%xx     cores   &       & & 10       & 4          & 2 $\times$ 7    & 10            & 20          & 64            &   8          &  24            \\
%xx     ISA     &       & & AVX      & AVX2       & AVX2            & AVX2          & AVX-512     & AVX-512       & AVX2         &  AVX2          \\
%xx %    sockets &       & 2        & 1          & 2               & 2             & 2           & 1             & 1            &  & 1 \\
%xx %    \mltwo{NUMA LDs}& 2        & 1          & 2 $\times$ 2    & 2 $\times$ 2  & 2           & 1             & 1            &  & 1 \\
%xx     \mltwo{NUMA LDs} & & 1       & 1          & 2               & 1             & 1           & 1             & 1            & 4              \\
%xx     \hline                                                    
%xx     L1 & [KiB]     &  &  32      & 32         & 32              & 32            & 32          & 32            & 32           &  32            \\
%xx     L2 & [KiB]     &  &  256     & 256        & 256             & 256           & 1024        & 1024          & 512          &  512           \\
%xx     L3 & [MiB]     &  &  25      & 8          & 2 $\times$ 17.5 & 25            & 28          & -             & 2 $\times$ 8 &  8 $\times$ 8 \\ 
%xx     \hline
%xx %    copy bw. & [GB/s] & 41.2   & 26.6            & 22.6       & 31.3 & ?? &  75.9 & 30.2 & 212 \\ % complete socket/cod
%xx %    read bw. & [GB/s] & 44.3   & 30.9            & 23.6       & 33.7 & ?? &  74.2 & 32.5 & 231 \\ % complete socket/cod
%xx     \mlfour{scalar read bw.}   &     &          &  \\
%xx     ~1 core  & [GB/s]  &&  9.5 & 16.6 & 12.1 & 11.5 &  14.5 &  8.5 & 19.3 & 19.3  \\
%xx     ~NUMA LD & [GB/s]  && 44.4 & 22.7 & 31.2 & 56.3 & 108.0 & 75.2 & 33.7 & 37.6  \\
%xx     \hline
%xx     \mlfour{scalar ADD+MUL/FMA} &&& \\
%xx     ~1 core  & [F/cy] &&  2 &  4 &  4 &  4 &  4 &   4 &  4 &  4 \\
%xx     ~NUMA LD & [F/cy] && 20 & 16 & 28 & 40 & 80 & 256 & 32 & 24 \\
%xx     \hline
%xx     \mlfour{scalar machine balance $B_m$} &  &         & \\
%xx     ~1 core  & [B/F] && 2.2 & 1.2 & 1.3 & 1.3 & 1.5 & 1.6 & 1.4 & 2.1 \\
%xx     ~NUMA LD & [B/F] && 1.0 & 0.4 & 0.5 & 0.6 & 0.6 & 0.2 & 0.3 & 0.7 \\
%xx     \hline
%xx \\[0.01em]
%xx   \end{tabular}
%xx %} % from https://tex.stackexchange.com/a/27105
%xx \end{table*}
%\parbox{.25\linewidth}{ 
%\begin{SCtable}[][t]
\begin{table*}[tp]
  \centering
  \small
  \begin{tabular}{ll|rrrrrr}
  \hline
  matrix      &&  \multicolumn{1}{c}{${n}$} &&
            \multicolumn{1}{c}{${\text{nnz}(A)}$}  &&
            \multicolumn{1}{c}{${\text{nnz}(L)}$}   \\ 
%  {\bfseries matrix}      &&  \multicolumn{1}{c}{$\bm{n}$} &&
%            \multicolumn{1}{c}{$\bm{\text{nnz}(A)}$}  &&
%            \multicolumn{1}{c}{$\bm{\text{nnz}(L)}$}   \\ 
  \hline
% values for threads = 1, p = 80
  dense  && $    20 \times 10^3$ && $200 \times 10^6$ &&  $  200 \times 10^6$  \\ % ps-n-20000-t-1-p-80
  lapl1  && $   256 \times 10^3$ && $  3 \times 10^6$ &&  $  219 \times 10^6$  \\ % pl-n-40-b-4-t-1-p-80  N=40, B=4
  lapl2  && $   343 \times 10^3$ && $  1 \times 10^6$ &&  $  166 \times 10^6$  \\ % pl-n-70-b-1-t-1-p-80  N=70, B=1
  omen1  && $1\,751 \times 10^3$ && $ 32 \times 10^6$ && $1\,076 \times 10^6$  \\ % omen-rc2.5-lc160-t-1-p-80
  omen2  && $   760 \times 10^3$ && $ 20 \times 10^6$ && $   690 \times 10^6$  \\ % omen-rc3.5-t-1-p-80
  omen3  && $1\,271 \times 10^3$ && $ 42 \times 10^6$ && $1\,651 \times 10^6$  \\ % omen-rc4.5-t-1-p-80
  bddc   && $   750 \times 10^3$ && $ 31 \times 10^6$ && $1\,590 \times 10^6$  \\ % mat\_Kii\_sd22\_size750141\_load2\_newton1
  \hline
  \end{tabular}
  \caption{Dimension ($n$) and number of nonzeros ($\text{nnz}$) for $A$ and
$L$ for all benchmark matrices.}
  \label{tab:m:list}
%}\vfill{}
%\end{table}
%\end{SCtable}
\end{table*}

For our performance predictions, we apply the roofline model~(\cite{williams-2009}).
The model takes into account the attainable memory bandwidth as well as 
the
peak floating
point performance of the processor and 
\uwave{%\sout{correlates} 
relates these hardware capabilities %\sout{with}
to
the requirements of the code.}
%mw2018-06-07: \mycomment{OR: was heisst dieses? MW: besser? OR: Etwas. Aber inwiefern wird
%mw2018-06-07: eine Beziehung zu den Anforderungen hergestellt? MW: Die Code-Balance $B_c$ ist
%mw2018-06-07: die Anforderung des Codes und $B$ bzw.\ $P_\text{max}$ stammt von der Hardware.}

It can be written as
%
\be
  P = \min(P_\text{max}, B / B_c),
\ee
where $P_\text{max}$ denotes the attainable floating point performance, $B$ the
attainable memory bandwidth, and $B_c$ the code balance.

Here, $P_\text{max}$
depends already on the floating point characteristics of the code and the
processor and does not represent the peak floating point performance as it can
be obtained from a processor's data sheet. 
If a processor supports vectorized FMA instructions, but
only vectorized add and multiply instructions are used, then $P_\text{max}$ is
halved.
Using scalar instructions instead of the AVX vectorized counterparts reduces
$P_\text{max}$ further by a factor of four.
%
And, finally, if the floating point instruction mix does not equally utilize a
processors floating point units, $P_\text{max}$ is again reduced.
% \todo{This is a forward reference.}
For example, the Ivy Bridge (IVB) system (section~\ref{sec:tb}, Table~\ref{tab:hw}) hosts an add and
multiply unit, but if only one type of floating point instruction is used, only
half of the theoretical floating
point performance can be attained. 
%
In our case the compiler uses 
scalar FMA instructions
for the core loops of the %\sout{forward/backward substitution} 
sparse triangular solve
for architectures
with AVX2 support and scalar add/multiply instructions for
architectures without FMA support like IVB.

The attainable memory bandwidth $B$ is measured with a microbenchmark,
ideally resembling the application's memory access pattern. 
For the
sparse triangular solve,
%\sout{\OR the forward/backward substitution}
we use a read-only benchmark where one
vector located in memory is summed up.
%
As the sparse triangular solve
%\sout{\OR the forward/backward substitution}
in PARDISO only uses scalar load instructions we also use them
for the microbenchmark. 

The \textit{code balance} $B_c$ is the ratio of bytes transferred to the
number of FLOPs performed in the code.
In the best case, when the panel size $\panelsize$ is large and the indices are
cached, during the forward~\eqref{eq:algo:fw:pardiso} and backward
substitutions~\eqref{eq:algo:bw:pardiso}, each nonzero of $L$ must be loaded once
and the loading of indices can be neglected,
respectively. 
Furthermore, the computation involves two FLOPs per nonzero.
As nonzeros are stored in double precision consuming $8$\,B, this results in a
best case code balance of $B_c = 8 / 2 \text{B/F} = 4 \text{B/F}$, where F
denotes FLOP.
%1 Table~\ref{tab:mrm:bc} lists the code balance for different unrollings and cache
%1 levels and uses the data transfer and FLOP counts from Sect.~\ref{sec:sds}.
Table~\ref{tab:mrm:bc} lists the code balance for different unrolling factors
when the factor $L$ must be fetched from memory and uses the data transfer and
FLOP counts from section~\ref{sec:sds}.

In contrast, the \textit{machine balance} $B_m$ defines the ratio for the whole system
and uses the ratio of the attainable memory bandwidth $B$ to the maximum floating point
performance $P_\text{max}$.
% \todo{This is a forward reference.}
% \mycomment{OR: Tab. ist keine erlaubte Abkuerzung f\"ur Table. MW: stimmt}
This is found in Table~\ref{tab:hw} for all systems.
%
If $B_c > B_m$ then the roofline model indicates 
that the code's performance is
limited by the memory bandwidth,
i.\,e., that the code is memory bound.
%1 This is the case for sparse direct solve for all unrollings, when data is
%1 located in L2, L3, or memory.
{This is the case for the sparse %\sout{direct} 
triangular solve 
phase %\sout{for} 
of all
%mw2018-06-07: \uwave{
unrolling factors,
%mw2018-06-07: }
when the factor $L$ is too large to be kept in cache and
completely located in memory.}
%mw2018-06-07: \mycomment{\OR was meint ``unrolling factor'' hier? \MW MW: Das ist der Faktor
%mw2018-06-07: f√ºr das ``loop unrolling''. Der Begriff wird vorher auch schon so verwendet,
%mw2018-06-07: daher haette ich den nicht nocheinmal erlaeutert.}

To determine performance limits, we only consider the case when data resides in
memory.
Therefore the data transfers between the L3 cache and memory are relevant as shown
in Figure~\ref{fig:ecm:data}.
Only nonzero entries of $L$ with the corresponding panel indices are loaded. 
Their amount depends only on the structure of $L$ and is independent of the used
loop unrollings.
%
The roofline model for the memory bound case as a function of the number of
threads~$t$ can be formulated as
%
\be
  \label{eq:rm:simple}
  P^{A}(t)
  = \frac{
      \text{nnz} (L) \times 2 \frac { \text{FLOP}}{ \text{nz}}
    }{
     \frac{D_A(t)}{ B(t)} 
    } \quad \frac{\text{FLOP}}{s},
\ee
%
%\mycomment{OR: Genaugenommen sind Einheiten in eckigen Klammern veraltet;
%https://de.wikipedia.org/wiki/Einheitenzeichen MW: OK, die Einheiten in Klammern
%in den Tabellen sollten aber passen.
%OR: Es ist mir nicht so wichtig, es kann auch in eckige Klammern.
%}
where $\text{nnz}(L)$ denotes the number of nonzeros of the factor $L$ 
resulting from a factorization of $A$ for $t$ threads, 
$B(t)$ is the attainable memory bandwidth of the system utilizing $t$ threads, and 
$D_A(t)$ the data volume of nonzeros and indices making up $L$.
The only adjustment to the original model here is its dependency on the number
of threads. 
Choosing $t$ equal to the number of total cores yields the original roofline model.

To distinguish between the parallel phase, where the parts are handled,
and the serial part, where the separator is treated, we modify 
\eqref{eq:rm:simple}.
We use the following formula for the \textit{modified roofline model}: 
%
\be
  \label{eq:rm:mod}
  P^{A}_{\text{mod}}(t) 
  = \frac{
      \text{nnz} (L) \times 2 \frac { \text{FLOP}}{ \text{nz}}
    }{
     \frac{D_A^p(t) }{  B(t) } + \frac{ D_A^s(t) }{ B(1) }
    } \quad \frac{\text{FLOP}}{s},
\ee
%
where
$D_A^p(t)$ 
represents
the data volume of nonzeros and indices %\sout{\uwave{made up}} 
built up
% \mycomment{OR: ``made up''? Erfunden? MW: OK}
by the parallel parts, and $D_A^s(t)$ the data volume %\sout{\uwave{made up}}
built up by the nonzeros and
indices of the separator.
%
Please note that both data volumes $D_A^p(t)$ and $D_A^s(t)$ depend on
$A$ and the number of threads $t$.
%
The values for $D_A^p(t)$ and $D_A^s(t)$ are extracted from the factorized
matrices a priori to solve.

Please note that the accuracy of the roofline model predictions, especially for
single cores, as included in the modified model, can be inaccurate. 
%\mycomment{OR: Auf welches Ganzes bezieht sich ``part''? MW: Done.}
If the in-core execution time (excluding floating point operations) or the in-cache
traffic dominates the execution time, predictions become unreliable as this is not
covered by the roofline model.
In section~\ref{sec:performance}, we use dedicated single core measurements to
validate that, in the case of the sparse triangular solve,
%\sout{\OR the forward/backward substitution,}
this approach is valid. 
%
However, this effect can be modeled in detail with the ECM
model (\cite{treibig-2010-ecm, hager-2012-ecm, stengel-2015}) and was already
studied for stencil kernels.


%-- \begin{table*}[!t]
%-- \parbox{.7\linewidth}{
%--   \caption{Details of Evaluated Hardware Systems.
%-- % ISA Lists the Latest Extension Supported by the Processor. 
%-- % Read Memory Bandwidth, Floating Point Instructions per Cycle (ADD+MULL and
%-- % FMA Instructions), and Machine Balance is Reported for Scalar Execution.
%-- KNL's Bandwidth Numbers are for DDR Memory.}
%-- % ECM: 2 cy L1/L2 bei HSW/BDW entgegen der Doku 
%-- % STREAM:
%-- % read = summation
%-- % KNL: no-nt, prefetch not explicitly disabled
%-- % ZEN: no-nt, AVX2, only even cores
%-- % HSW2: no-nt, avx2
%-- % HSW: no-nt, avx2
%-- % IVB: no-nt, 
%--   \label{tab:hw}
%--   \footnotesize
%--  \centering
%-- %\resizebox{\textwidth}{!}{%
%--  \begin{tabular}{p{1.9cm}llrrrrrrrrr}
%--     \hline
%--     name      & &  & IVB         & HSW-D      & HSW-S           & BDW           & SKX         & KNL           & ZEN-D        &  ZEN-S         \\
%--     \hline                                                      
%--     \multirow{3}{\linewidth}{processor name} & &  & Intel& Intel & Intel & Intel & Intel & Intel & AMD &  AMD \\
%--       & &  & Xeon  & Xeon & Xeon      & Xeon    & Xeon  & Xeon    &
%-- ~~Ryzen 7   &  ~~EPYC      \\
%--               & &  &\scriptsize  E5-2660 v2 &\scriptsize  E3-1240 v3
%-- &\scriptsize  E5-2695 v3      &\scriptsize  E5-2630 v4    &\scriptsize  Gold
%-- 6148   &\scriptsize  Phi 7210      &\scriptsize  1700X      &\scriptsize
%-- 745           \\
%-- %     processor & &  & Intel Xeon  & Intel Xeon & Intel Xeon      & Intel Xeon    & Intel Xeon  & Intel Xeon    & AMD Ryzen    &  AMD EPYC      \\
%-- %     name      & &  &  E5-2660 v2 & E3-1240 v3 & E5-2695 v3      & E5-2630 v4    & Gold 6148   & Phi 7210      & 7 1700X      &  745           \\
%--     \hline                                                      
%--     micro     & &  & Ivy Bridge  & Haswell    & Haswell         & Broadwell
%-- & Skylake     & ~Knigths       & Zen          &  Zen           \\
%--     arch.     & &  &             &            &                 &               &             & Landing       & \\
%--     \hline                                                    
%--     freq    & [GHz] & & 2.2      & 3.4        & 2.3             & 2.2           & 2.4         & $\approx$ 1.3 & 3.4          &  2.3           \\
%--     cores   &       & & 10       & 4          & 2 $\times$ 7    & 10            & 20          & 64            &   8          &  24            \\
%--     ISA     &       & & AVX      & AVX2       & AVX2            & AVX2          & AVX-512     & AVX-512       & AVX2         &  AVX2          \\
%-- %    sockets &       & 2        & 1          & 2               & 2             & 2           & 1             & 1            &  & 1 \\
%-- %    \mltwo{NUMA LDs}& 2        & 1          & 2 $\times$ 2    & 2 $\times$ 2  & 2           & 1             & 1            &  & 1 \\
%--     \mltwo{NUMA LDs} & & 1       & 1          & 2               & 1             & 1           & 1             & 1            & 4              \\
%--     \hline                                                    
%--     L1 & [KiB]     &  &  32      & 32         & 32              & 32            & 32          & 32            & 32           &  32            \\
%--     L2 & [KiB]     &  &  256     & 256        & 256             & 256           & 1024        & 1024          & 512          &  512           \\
%--     L3 & [MiB]     &  &  25      & 8          & 2 $\times$ 17.5 & 25            & 28          & -             & 2 $\times$ 8 &  8 $\times$ 8 \\ 
%--     \hline
%-- %    copy bw. & [GB/s] & 41.2   & 26.6            & 22.6       & 31.3 & ?? &  75.9 & 30.2 & 212 \\ % complete socket/cod
%-- %    read bw. & [GB/s] & 44.3   & 30.9            & 23.6       & 33.7 & ?? &  74.2 & 32.5 & 231 \\ % complete socket/cod
%--     \mlfour{scalar read bw.}   &     &          &  \\
%--     ~1 core  & [GB/s]  &&  9.5 & 16.6 & 12.1 & 11.5 &  14.5 &  8.5 & 19.3 & 19.3  \\
%--     ~NUMA LD & [GB/s]  && 44.4 & 22.7 & 31.2 & 56.3 & 108.0 & 75.2 & 33.7 & 37.6  \\
%--     \hline
%--     \mlfour{scalar ADD+MUL/FMA} &&& \\
%--     ~1 core  & [F/cy] &&  2 &  4 &  4 &  4 &  4 &   4 &  4 &  4 \\
%--     ~NUMA LD & [F/cy] && 20 & 16 & 28 & 40 & 80 & 256 & 32 & 24 \\
%--     \hline
%--     \mlfour{scalar machine balance $B_m$} &  &         & \\
%--     ~1 core  & [B/F] && 2.2 & 1.2 & 1.3 & 1.3 & 1.5 & 1.6 & 1.4 & 2.1 \\
%--     ~NUMA LD & [B/F] && 1.0 & 0.4 & 0.5 & 0.6 & 0.6 & 0.2 & 0.3 & 0.7 \\
%--     \hline
%-- \\[0.01em]
%--   \end{tabular}
%-- %} % from https://tex.stackexchange.com/a/27105
%-- }
%-- %\end{table*}
%-- \parbox{.25\linewidth}{ 
%-- %\begin{SCtable}[][t]
%-- %\begin{table}[tp]
%--   \caption{Dimension ($n$) and number of nonzeros ($\text{nnz}$) for $A$ and 
%-- $L$ for all benchmark matrices.}
%--   \label{tab:m:list}
%--   \centering
%--   \small
%--   \begin{tabular}{ll|rrrrrr}
%--   \hline
%--   matrix      &&  \multicolumn{1}{c}{${n}$} &&
%--             \multicolumn{1}{c}{${\text{nnz}(A)}$}  &&
%--             \multicolumn{1}{c}{${\text{nnz}(L)}$}   \\ 
%-- %  {\bfseries matrix}      &&  \multicolumn{1}{c}{$\bm{n}$} &&
%-- %            \multicolumn{1}{c}{$\bm{\text{nnz}(A)}$}  &&
%-- %            \multicolumn{1}{c}{$\bm{\text{nnz}(L)}$}   \\ 
%--   \hline
%-- % values for threads = 1, p = 80
%--   dense  && $    20 \times 10^3$ && $200 \times 10^6$ &&  $  200 \times 10^6$  \\ % ps-n-20000-t-1-p-80
%--   lapl1  && $   256 \times 10^3$ && $  3 \times 10^6$ &&  $  219 \times 10^6$  \\ % pl-n-40-b-4-t-1-p-80  N=40, B=4
%--   lapl2  && $   343 \times 10^3$ && $  1 \times 10^6$ &&  $  166 \times 10^6$  \\ % pl-n-70-b-1-t-1-p-80  N=70, B=1
%--   omen1  && $1\,751 \times 10^3$ && $ 32 \times 10^6$ && $1\,076 \times 10^6$  \\ % omen-rc2.5-lc160-t-1-p-80
%--   omen2  && $   760 \times 10^3$ && $ 20 \times 10^6$ && $   690 \times 10^6$  \\ % omen-rc3.5-t-1-p-80
%--   omen3  && $1\,271 \times 10^3$ && $ 42 \times 10^6$ && $1\,651 \times 10^6$  \\ % omen-rc4.5-t-1-p-80
%--   bddc   && $   750 \times 10^3$ && $ 31 \times 10^6$ && $1\,590 \times 10^6$  \\ % mat\_Kii\_sd22\_size750141\_load2\_newton1
%--   \hline
%--   \end{tabular}
%-- }\vfill{}
%-- %\end{table}
%-- %\end{SCtable}
%-- \end{table*}
