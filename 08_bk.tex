\chapter{\todol{Conclusion} and Research Roadmap}
\label{sec:conclusion}

In this paper, we analyzed PARDISO's sparse triangular solve algorithm regarding data
transfers and floating point operations.
This serves as input for our modified Roofline performance model.
The model covers the serial and parallel execution phases of the forward and
backward subst.\ and in general captures the behavior of the measured
performance.
%
For Intel Ivy Bridge and Haswell (desktop) systems as well as AMD Zen platforms
the model error is only up to $20$\,\%.
However, for Intel Haswell (server), Broadwell, and Skylake (server) the model
error raises up to up to $60$\,\% depending on the system and matrix.
%
The observed deviations require a deeper analysis
of the architectures
and the algorithm. 
Here, the ECM performance
model may be a suitable tool. 
%mw2018-06-07: \mycomment{ML: War ECM hier nicht eher ungenauer
%mw2018-06-07: in der Vorhersage? MW: OK, erklärung eingefügt.} 
This model additionally accounts for in-cache traffic and the complete in-core
execution.
This advanced model delivers excellent predictions for vectorized code. 
However, modeling scalar code execution, as it is used in sparse triangular solve, 
is still under development.
%
For Knights Landing initial deviations of the model 
of
up to $130$\,\% 
result from the bottleneck given by the L2 bandwidth.
By
adjustments, the error can be reduced, however with multiple cores
the situation becomes more complex as L2 or memory bandwidth can be bottlenecks.
%
%
Sparse triangular solve uses only scalar loads and stores, but with one core
of the evaluated AMD Zen systems, it achieves nearly the same bandwidth as
obtained with the vectorized read-only benchmark. For this % \sout{obersation} 
observation currently
no explanation can be provided.
%
%mw2018-06-07: \mycomment{OR: Ist das nicht notwendig fuer jeden Algorithmus so? (Theorem ueber
%mw2018-06-07: Nested dissection) MW: da bin ich ueberfragt.}
In the current factorization algorithm the serial fraction of the factor $L$
increases with the number of cores.
As a result, typically the highest performance for the sparse
triangular solve phase is already reached with four or eight cores.
%
PARDISO's high performance sparse triangular solve favors hardware with a high
memory bandwidth that can ideally be saturated with one or two cores. %, 
%\uwave{as it is
%typically the case with current desktop systems or matrices where the serial
%fraction increases slowly over the number of cores.}

% This is achieved by desktop systems.
% This is achieved by either vector machines like SX ACE or at lower bandwidths by
% desktop systems.