\chapter{Conclusion}
\label{sec:conclusion}

PARDISO and other state-of-the-art sparse direct solvers utilize various techniques to improve performance. First, a reordering of the matrix is found. A~graph partitioning algorithm, called nested disection, produces a reordering that minimizes fill-in and provides a high level of concurrency in factorization. Then, the matrix is factorized using $LU$, $LDL^T$, or $LL^T$ algorithm. Last, the solution is obtained by forward and backward substitution. In this thesis we focus on the last step, the forward and backward substitution. Certain applications require solving many times the same linear system $A$ with different right-hand sides, and thus the forward and backward substitution is essential for performance of these applications. Such appliations are, for example, FETI domain decomposition methods or certain optimization problems.

The Berkeley roofline model is widely used to visualize the performance of executed code together with the upper performance bounds given by the memory bandwidth and the processor peak performance. The model can hereby provide an insightful visualization of bottlenecks. In this thesis a modification of the roofline model that allows us to cover combination of serial and parallel execution is introduced. This modified form of the roofline model is then aplied to the triangular solve step of PARDISO. The performance of the forward and backward substitution process has been analyzed and benchmarked for a representative set of sparse matrices on various modern x86-type multicore architectures and the Knights Landing manycore architecture. It has been shown how to also accurately measure the necessary quantities, such as the achievable memory bandwidth, for threaded code. The measurement approach, its validation, as well as limitations were discussed.
For Intel Ivy Bridge and Haswell (desktop) systems as well as AMD Zen platforms the model error is only up to $20\,\%$. However, for the Intel Haswell (server), Broadwell, and Skylake (server) the model error raises up to $60\,\%$ depending on the system and matrix. The observed deviations require a deeper analysis
of the architectures and the algorithm. For Knights Landing the initial deviations of the model
of up to $130\,\%$ result from the bottleneck given by the L2 bandwidth.
By adjustments, the error can be reduced; however, with multiple cores the situation becomes more complex as L2 or memory bandwidth can be bottlenecks. Sparse triangular solve uses only scalar loads and stores, but with one core of the evaluated AMD Zen systems, it achieves nearly the same bandwidth as obtained with the vectorized read-only benchmark. Besides the Berkeley roofline model we also used the Erlangen ECM model. This model additionally accounts for in-cache traffic and the complete in-core execution. This advanced model delivers excellent predictions for vectorized code. However, modeling scalar code execution, as it is used in sparse triangular solve, is still under development. For the scalar code of sparse triangular solve we got a reasonable upper and lower bound of performance, but we did not achieve such good predictions as for the vecotrized codes.

In the current factorization algorithm, the serial fraction of the factor $L$
increases with the number of cores. As a result, the highest performance for the sparse triangular solve phase is, typically, reached with four or eight cores.
PARDISO's high performance sparse triangular solve favors hardware with a high
memory bandwidth that can ideally be saturated with one or two cores.
