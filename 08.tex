\chapter{Conclusion and Research Roadmap}
\label{sec:conclusion}

We analyzed the sparse triangular solve algorithm regarding data
transfers and floating point operations.
This serves as input for our modified roofline performance model.
The model covers the serial and parallel execution phases of the forward and
backward substitutions and in general captures the behavior of the measured
performance.
%
For Intel Ivy Bridge and Haswell (desktop) systems as well as AMD Zen platforms
the model error is only up to $20$\,\%.
However, for Intel Haswell (server), Broadwell, and Skylake (server) the model
error raises up to $60$\,\% depending on the system and matrix.
%
%The observed deviations require a deeper analysis
%of the architectures
%and the algorithm. 
The observed deviations require a deeper analysis
of the architectures
and the algorithm.

The analysis of sparse triangular solve algorithm and modeling with the modified roofline model was published in %\cite{wittmann-sbac-pad-2018}.

\vspace{0.5em}
M. Wittmann, G. Hager, R. Janalik, M. Lanser, A. Klawonn, O. Rheinbach, O. Schenk, and G. Wellein. Multicore performance engineering of sparse triangular solves using a modified roofline model. \textit{In 2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)}, pages 233-241, 2018.
\vspace{0.5em}

The rest of my PhD studies will be focused on modeling forward and backward substitutions with the ECM model, which could deliver better predictions than the roofline model.
The roofline model is based only on memory traffic and peak performance.
On the contrary, the ECM model also takes into account in-cache traffic and in-core execution, therefore, it may be a suitable tool for better predictions. This model was originally designed for modeling stencil codes, but was successfully used also for modeling more complicated codes like neuron simulation from the Blue Brain project by \cite{Cremonesi-2020}. This code, unlike a simple stencil, contains indirect memory accesses and instructions like exp() or divide, which take more cycles than addition or multiplication.

In Tables \ref{tab:publications}, \ref{tab:ta}, and \ref{tab:courses} there is a summary of my publications, teaching activities, and grades.

\begin{table}
\caption{Publications.}
\label{tab:publications}
M. Wittmann, G. Hager, R. Janalik, M. Lanser, A. Klawonn, O. Rheinbach, O. Schenk, and G. Wellein. Multicore performance engineering of sparse triangular solves using a modified roofline model. \textit{In 2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)}, pages 233-241, 2018.
\vspace{0.5em} \\
A. Klawonn, M. Lanser, M. Uran, O. Rheinbach, O. Schenk, G. Wellein, J. Schr\"oder, and D. Balzani, R. Janalik,
Towards A Virtual Laboratory - Computation of Forming Limit Curves
Lecture Notes in Computational Science and Engineering, \textit{Springer}:1-42, accepted, in press.
\vspace{0.5em} \\
M. Bollh\"ofer, O. Schenk , R. Janalik, S. Hamm, and K. Gullapalli.
State-of-The-Art Sparse Direct Solvers
Parallel Algorithms in Computational Science&Engineering -- Parallelism as Enabling Technology in CSE Applications, \textit{Birkhauser}, accepted, in press, https://arxiv.org/abs/1907.05309.
\vspace{0.2em}
\hline
\end{table}

\begin{table}[h!]
\centering
\caption{Courses served as a teaching assistant.}
\label{tab:ta}
\begin{tabular}{lllll}
\hline
Course                               & Instructor             & Program & Semester  \\ \hline
Software Atelier: Supercomputing & Olaf Schenk & MCS & Spring 15/16 \\
Node-Level Performance Engineering & G. Wellein, G. Hager & MCS & Spring 15/16 \\
High-Performance Computing & Olaf Schenk & MCS & Fall 16/17 \\
Software Atelier: Supercomputing & Olaf Schenk & MCS & Spring 16/17 \\
CSCS-USI Summer School & CSCS staff \& USI & PhD & Spring 16/17 \\
High-Performance Computing & Olaf Schenk & MCS & Fall 17/18 \\
Software Atelier: Supercomputing & Olaf Schenk & MCS & Spring 17/18 \\
CSCS-USI Summer School & CSCS staff \& USI & PhD & Spring 17/18 \\
High-Performance Computing & Olaf Schenk & MCS & Fall 18/19 \\
CSCS-USI Summer School & CSCS staff \& USI & MCS & Spring 18/19 \\
High-Performance Computing & Olaf Schenk & MCS & Fall 19/20	\\ \hline
 \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Breadth requirement overview.}
\label{tab:courses}
\begin{tabular}{lllll}
\hline
Course                           & Grade & ECTS & Semester & Type  \\ \hline
Deterministic Methods       & 8.0 & 4 & Fall 15/16 & MSc X \\
Numerical Algorithms        & 9.0 & 2 & Fall 15/16 & MSc X \\
Stochastic Methods          & 8.5 & 4 & Spring 15/16 & MSc X \\
Introduction to Doctoral Studies & pass & 2 & Spring 15/16 & PhD \\
Quantum Computing           & pass & 4 & Spring	17/18 & MSI \\
\hline
Research Prospectus &  pass    & -    & Jan 2018  & PhD   \\ \hline
 \hline
\end{tabular}
\end{table}

%The observed deviations require a 
%deeper analysis
%of the architectures
%and the algorithm. 
%ECM performance model. \todol{}
%%Here, the ECM performance
%%model may be a suitable tool. 
%%mw2018-06-07: \mycomment{ML: War ECM hier nicht eher ungenauer
%%mw2018-06-07: in der Vorhersage? MW: OK, erklärung eingefügt.} 
%This model additionally accounts for in-cache traffic and the complete in-core
%execution.
%This advanced model delivers excellent predictions for stencil codes. 
%However, modeling scalar code execution, as it is used in sparse triangular solve, 
%is still under development.
%%
%For Knights Landing initial deviations of the model 
%of
%up to $130$\,\% 
%result from the bottleneck given by the L2 bandwidth.
%By
%adjustments, the error can be reduced, however with multiple cores
%the situation becomes more complex as L2 or memory bandwidth can be bottlenecks.
%%
%%
%Sparse triangular solve uses only scalar loads and stores, but with one core
%of the evaluated AMD Zen systems, it achieves nearly the same bandwidth as
%obtained with the vectorized read-only benchmark. For this % \sout{obersation} 
%observation currently
%no explanation can be provided.
%
%%mw2018-06-07: \mycomment{OR: Ist das nicht notwendig fuer jeden Algorithmus so? (Theorem ueber
%%mw2018-06-07: Nested dissection) MW: da bin ich ueberfragt.}
%In the current factorization algorithm the serial fraction of the factor $L$
%increases with the number of cores.
%As a result, typically the highest performance for the sparse
%triangular solve phase is already reached with four or eight cores.
%%
%PARDISO's high performance sparse triangular solve favors hardware with a high
%memory bandwidth that can ideally be saturated with one or two cores. %, 
%%\uwave{as it is
%%typically the case with current desktop systems or matrices where the serial
%%fraction increases slowly over the number of cores.}
%
%% This is achieved by desktop systems.
%% This is achieved by either vector machines like SX ACE or at lower bandwidths by
%% desktop systems.
